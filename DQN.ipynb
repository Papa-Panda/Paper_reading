{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOShl5AMOPHJ11NlmiAd1Yo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://chatgpt.com/c/67a6f8e8-31cc-800e-b227-052ea4820b5e"
      ],
      "metadata": {
        "id": "fiKWEcWrUhL2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDXNZMRWVcGY",
        "outputId": "1c939f3c-5949-4655-d342-a0fd11d3fd0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env_specs = gym.registry.keys()\n",
        "print(list(env_specs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhxM-f0hV751",
        "outputId": "9bde1da4-b630-423a-e619-794dbe00939f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# 创建 CartPole 环境\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "state_size = env.observation_space.shape[0]  # 状态维度\n",
        "action_size = env.action_space.n  # 动作数量\n",
        "\n",
        "# 超参数\n",
        "gamma = 0.99  # 折扣因子\n",
        "epsilon = 1.0  # 初始探索率\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "memory_size = 10000\n",
        "target_update_freq = 10  # 目标网络更新频率\n",
        "episodes = 500  # 训练回合数\n",
        "\n",
        "# 经验回放缓冲区\n",
        "memory = deque(maxlen=memory_size)\n",
        "\n",
        "# 记录奖励\n",
        "reward_history = []\n",
        "\n",
        "# DQN 网络\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(24, activation='relu', input_shape=(state_size,)),\n",
        "        keras.layers.Dense(24, activation='relu'),\n",
        "        keras.layers.Dense(action_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "    return model\n",
        "\n",
        "# 初始化网络\n",
        "model = build_model()\n",
        "target_model = build_model()\n",
        "target_model.set_weights(model.get_weights())  # 复制初始权重\n",
        "\n",
        "# 画图函数\n",
        "def plot_frame(frame):\n",
        "    plt.imshow(frame)\n",
        "    plt.axis(\"off\")\n",
        "    display(plt.gcf())\n",
        "    clear_output(wait=True)\n",
        "\n",
        "# 训练 DQN\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(500):  # CartPole-v1 最多 500 步\n",
        "        # 选择动作（ε-贪心）\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            q_values = model.predict(state, verbose=0)\n",
        "            action = np.argmax(q_values[0])\n",
        "\n",
        "        # 采取动作\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        # 存储经验\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 渲染并可视化\n",
        "        frame = env.render()\n",
        "        plot_frame(frame)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    reward_history.append(total_reward)  # 记录每轮奖励\n",
        "\n",
        "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "    # 经验回放训练\n",
        "    if len(memory) > batch_size:\n",
        "        minibatch = random.sample(memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target_q = model.predict(state, verbose=0)\n",
        "            if done:\n",
        "                target_q[0][action] = reward\n",
        "            else:\n",
        "                future_q = np.max(target_model.predict(next_state, verbose=0))\n",
        "                target_q[0][action] = reward + gamma * future_q\n",
        "            model.fit(state, target_q, epochs=1, verbose=0)\n",
        "\n",
        "    # 目标网络更新\n",
        "    if episode % target_update_freq == 0:\n",
        "        target_model.set_weights(model.get_weights())\n",
        "\n",
        "    # 探索率衰减\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "env.close()  # 训练完成后关闭环境\n",
        "\n",
        "# 绘制奖励趋势图\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(reward_history, label=\"Total Reward per Episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"CartPole DQN Learning Progress\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Q9C6bMiwVU6T",
        "outputId": "48aa3635-ffe1-4cba-cef5-b0f88306ed52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BY8lnLU-Y52x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}