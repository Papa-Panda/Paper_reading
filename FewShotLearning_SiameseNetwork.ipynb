{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/FewShotLearning_SiameseNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YoOgiWVdGBpn"
      },
      "outputs": [],
      "source": [
        "#【Few-Shot Learning (1/3): 基本概念】 https://www.bilibili.com/video/BV1V44y1r7cx/?share_source=copy_web# https://chatgpt.com/c/671a643c-22b0-800e-b834-c4fa9ad90032"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kHcLJfs_GI22"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EH_BMCxAGR-8"
      },
      "outputs": [],
      "source": [
        "# class SiameseDataset(Dataset):\n",
        "class SiameseMNISTDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset  # MNIST dataset\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create a dictionary to quickly access all samples by class\n",
        "        self.class_to_indices = self._group_by_class()\n",
        "\n",
        "    def _group_by_class(self):\n",
        "        class_dict = {}\n",
        "        for idx, (_, label) in enumerate(self.dataset):\n",
        "            if label not in class_dict:\n",
        "                class_dict[label] = []\n",
        "            class_dict[label].append(idx)\n",
        "        return class_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, label1 = self.dataset[idx]\n",
        "\n",
        "        # Decide if the pair should be positive (same class) or negative (different class)\n",
        "        is_same = random.randint(0, 1)\n",
        "\n",
        "        if is_same:\n",
        "            # Positive pair: Select another image from the same class\n",
        "            img2_idx = random.choice(self.class_to_indices[label1])\n",
        "            label = 1.0\n",
        "        else:\n",
        "            # Negative pair: Select a random image from a different class\n",
        "            other_label = random.choice([x for x in range(10) if x != label1])\n",
        "            img2_idx = random.choice(self.class_to_indices[other_label])\n",
        "            label = 0.0\n",
        "\n",
        "        img2, _ = self.dataset[img2_idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return (img1, img2), torch.tensor(label, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4vRwzPMiGX4a"
      },
      "outputs": [],
      "source": [
        "def siamese_network(input_shape):\n",
        "    shared_network = create_shared_network(input_shape)\n",
        "\n",
        "    # Define the two inputs\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # Get feature vectors from the shared network\n",
        "    encoded_a = shared_network(input_a)\n",
        "    encoded_b = shared_network(input_b)\n",
        "\n",
        "    # Compute the L1 distance between the two feature vectors\n",
        "    l1_distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([encoded_a, encoded_b])\n",
        "\n",
        "    # Add a dense layer with sigmoid activation to get the similarity score\n",
        "    outputs = Dense(1, activation='sigmoid')(l1_distance)\n",
        "\n",
        "    return Model(inputs=[input_a, input_b], outputs=outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a11bIKAzGaZs"
      },
      "outputs": [],
      "source": [
        "# 3\n",
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, size=1000, transform=None):\n",
        "        self.size = size  # Total number of pairs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly generate two 1D \"images\" (vector) of size 10\n",
        "        image1 = torch.randn(10)\n",
        "        label = random.randint(0, 1)  # 1 for \"same\", 0 for \"different\"\n",
        "\n",
        "        if label == 1:\n",
        "            # Positive pair: Same class (for simplicity, similar vector)\n",
        "            image2 = image1 + torch.randn(10) * 0.1\n",
        "        else:\n",
        "            # Negative pair: Different classes (random vector)\n",
        "            image2 = torch.randn(10)\n",
        "\n",
        "        if self.transform:\n",
        "            image1 = self.transform(image1)\n",
        "            image2 = self.transform(image2)\n",
        "\n",
        "        return (image1, image2), torch.tensor(label, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xt-rdpfgGdVT"
      },
      "outputs": [],
      "source": [
        "def generate_pairs(data, labels, num_classes):\n",
        "    pairs = []\n",
        "    labels_pair = []\n",
        "\n",
        "    # Create positive and negative pairs\n",
        "    for class_idx in range(num_classes):\n",
        "        class_indices = np.where(labels == class_idx)[0]\n",
        "        np.random.shuffle(class_indices)\n",
        "\n",
        "        # Generate positive pairs\n",
        "        for i in range(len(class_indices) - 1):\n",
        "            pairs += [[data[class_indices[i]], data[class_indices[i + 1]]]]\n",
        "            labels_pair += [1]\n",
        "\n",
        "        # Generate negative pairs\n",
        "        other_class = (class_idx + 1) % num_classes\n",
        "        other_class_indices = np.where(labels == other_class)[0]\n",
        "        pairs += [[data[class_indices[0]], data[other_class_indices[0]]]]\n",
        "        labels_pair += [0]\n",
        "\n",
        "    return np.array(pairs), np.array(labels_pair)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y8JoNx3cGiLD"
      },
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),  # Input: (B, 1, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # Output: (B, 32, 14, 14)\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  # Output: (B, 64, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  # Output: (B, 64, 7, 7)\n",
        "        )\n",
        "\n",
        "        # Calculate the flattened size of the output: 64 * 7 * 7 = 3136\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Pass both images through the same convolutional layers\n",
        "        out1 = self.conv(x1)  # Shape: (B, 64, 7, 7)\n",
        "        out2 = self.conv(x2)  # Shape: (B, 64, 7, 7)\n",
        "\n",
        "        # Flatten the outputs to (B, 3136)\n",
        "        out1 = out1.view(out1.size(0), -1)\n",
        "        out2 = out2.view(out2.size(0), -1)\n",
        "\n",
        "        # Encode both images using the fully connected layers\n",
        "        out1 = self.fc(out1)\n",
        "        out2 = self.fc(out2)\n",
        "\n",
        "        # Return the L1 distance between the two encodings\n",
        "        return torch.abs(out1 - out2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P5fKzexmGmSr"
      },
      "outputs": [],
      "source": [
        "# 4\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, distance, label):\n",
        "        pos_loss = label * torch.pow(distance, 2)\n",
        "        neg_loss = (1 - label) * torch.pow(torch.clamp(self.margin - distance, min=0), 2)\n",
        "        loss = 0.5 * torch.mean(pos_loss + neg_loss)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij3FkL8AmK3I",
        "outputId": "dfd655c3-3eb6-43ac-b33e-bb35ee1c387e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 492kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.21MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the train and test datasets\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create the Siamese Datasets\n",
        "train_loader = DataLoader(SiameseMNISTDataset(train_dataset), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(SiameseMNISTDataset(test_dataset), batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dADOC1BDm6Zx",
        "outputId": "ee31c894-3499-4ea1-f8e5-9c185b366245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It takes  5 minutes.\n",
            "Epoch [1/10], Loss: 0.0230\n",
            "It takes  9 minutes.\n",
            "Epoch [2/10], Loss: 0.0076\n",
            "It takes  14 minutes.\n",
            "Epoch [3/10], Loss: 0.0060\n",
            "It takes  19 minutes.\n",
            "Epoch [4/10], Loss: 0.0053\n",
            "It takes  23 minutes.\n",
            "Epoch [5/10], Loss: 0.0042\n",
            "It takes  28 minutes.\n",
            "Epoch [6/10], Loss: 0.0037\n",
            "It takes  33 minutes.\n",
            "Epoch [7/10], Loss: 0.0036\n",
            "It takes  38 minutes.\n",
            "Epoch [8/10], Loss: 0.0031\n",
            "It takes  43 minutes.\n",
            "Epoch [9/10], Loss: 0.0031\n",
            "It takes  47 minutes.\n",
            "Epoch [10/10], Loss: 0.0028\n"
          ]
        }
      ],
      "source": [
        "# 6\n",
        "model = SiameseNetwork()\n",
        "criterion = ContrastiveLoss(margin=1.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for (img1, img2), label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        distance = model(img1, img2).norm(p=1, dim=1)\n",
        "        # Compute loss\n",
        "        loss = criterion(distance, label)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print('It takes ',round((time.time() - start_time)/60.0), 'minutes.')\n",
        "    print(f'Epoch [{epoch+1}/10], Loss: {total_loss / len(train_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"siamese_model.pth\")"
      ],
      "metadata": {
        "id": "ioXu6beRL58p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P6yhkoY5m81C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752d7718-6f6e-4055-9336-29d1bf3e30e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.69%\n"
          ]
        }
      ],
      "source": [
        "# 7\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (img1, img2), label in data_loader:\n",
        "            distance = model(img1, img2).norm(p=1, dim=1)\n",
        "            prediction = (distance < 0.5).float()  # Threshold at 0.5\n",
        "            correct += (prediction == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Evaluate on the test dataset\n",
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L0Nacv-nn_vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "db9b51f0-09ea-47b2-b52d-a9d751e8ad65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Image Label: 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 1, 28, 28]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9ca429ed0220>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Perform 1-shot learning inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mfew_shot_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-9ca429ed0220>\u001b[0m in \u001b[0;36mfew_shot_inference\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mquery_img_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, 1, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_img_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_img_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Query Image Label: {query_label}, Distance: {distance:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d4850650b54d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Pass both images through the same convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (B, 64, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (B, 64, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 1, 28, 28]"
          ]
        }
      ],
      "source": [
        "# few shot example\n",
        "def few_shot_inference(model):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Load MNIST test dataset\n",
        "    test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Pick a random image from the test set as the \"support\" image\n",
        "    support_idx = random.randint(0, len(test_dataset) - 1)\n",
        "    support_img, support_label = test_dataset[support_idx]\n",
        "\n",
        "    print(f\"Support Image Label: {support_label}\")\n",
        "\n",
        "    # Display the support image\n",
        "    plt.imshow(support_img.squeeze(), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "    # Generate a few query pairs (new images to compare against the support image)\n",
        "    for i in range(5):\n",
        "        query_idx = random.randint(0, len(test_dataset) - 1)\n",
        "        query_img, query_label = test_dataset[query_idx]\n",
        "\n",
        "        # Use the model to compute the similarity score\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            support_img_batch = support_img.unsqueeze(0).unsqueeze(0)  # (1, 1, 28, 28)\n",
        "            query_img_batch = query_img.unsqueeze(0).unsqueeze(0)  # (1, 1, 28, 28)\n",
        "\n",
        "            distance = model(support_img_batch, query_img_batch).norm(p=1, dim=1).item()\n",
        "\n",
        "        print(f\"Query Image Label: {query_label}, Distance: {distance:.4f}\")\n",
        "\n",
        "        plt.imshow(query_img.squeeze(), cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "        if distance < 0.5:\n",
        "            print(\"Prediction: Same\")\n",
        "        else:\n",
        "            print(\"Prediction: Different\")\n",
        "\n",
        "# Perform 1-shot learning inference\n",
        "few_shot_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ud9vexYTqUXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj6IP8gVsFTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}