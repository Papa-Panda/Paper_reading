{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUnlpXskZqk1nuhoIVYeD8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Deepseek_r1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "HOeZaV5HxcP_",
        "outputId": "3856d47b-9342-4b94-be9b-593253d552a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'deepspeed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc934d4054be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the grid environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import deepspeed\n",
        "\n",
        "# Define the grid environment\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.state = (0, 0)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        if action == 0 and x > 0:  # Up\n",
        "            x -= 1\n",
        "        elif action == 1 and x < self.size - 1:  # Down\n",
        "            x += 1\n",
        "        elif action == 2 and y > 0:  # Left\n",
        "            y -= 1\n",
        "        elif action == 3 and y < self.size - 1:  # Right\n",
        "            y += 1\n",
        "\n",
        "        self.state = (x, y)\n",
        "        reward = 1 if self.state == self.goal else -0.1\n",
        "        done = self.state == self.goal\n",
        "        return self.state, reward, done\n",
        "\n",
        "# Define the DQN model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Hyperparameters\n",
        "GRID_SIZE = 5\n",
        "STATE_DIM = 2\n",
        "ACTION_DIM = 4\n",
        "EPISODES = 500\n",
        "GAMMA = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_DECAY = 0.995\n",
        "MIN_EPSILON = 0.1\n",
        "BATCH_SIZE = 32\n",
        "MEMORY_SIZE = 10000\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = []\n",
        "        self.size = size\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "# Initialize environment, model, and buffer\n",
        "env = GridEnvironment(GRID_SIZE)\n",
        "model = DQN(STATE_DIM, ACTION_DIM)\n",
        "buffer = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "# DeepSpeed configuration\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"Adam\",\n",
        "        \"params\": {\n",
        "            \"lr\": LEARNING_RATE\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize DeepSpeed\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=deepspeed_config)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    state = torch.FloatTensor(state).unsqueeze(0).to(model_engine.device)\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        if np.random.rand() < EPSILON:\n",
        "            action = np.random.randint(ACTION_DIM)\n",
        "        else:\n",
        "            q_values = model_engine(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(model_engine.device)\n",
        "        buffer.add((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(buffer.buffer) >= BATCH_SIZE:\n",
        "            batch = buffer.sample(BATCH_SIZE)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.cat(states).to(model_engine.device)\n",
        "            actions = torch.LongTensor(actions).unsqueeze(1).to(model_engine.device)\n",
        "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(model_engine.device)\n",
        "            next_states = torch.cat(next_states).to(model_engine.device)\n",
        "            dones = torch.FloatTensor(dones).unsqueeze(1).to(model_engine.device)\n",
        "\n",
        "            q_values = model_engine(states).gather(1, actions)\n",
        "            next_q_values = model_engine(next_states).max(1)[0].unsqueeze(1)\n",
        "            target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
        "\n",
        "            loss = nn.MSELoss()(q_values, target_q_values)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    EPSILON = max(EPSILON * EPSILON_DECAY, MIN_EPSILON)\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {EPSILON:.2f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ei33S3x3xj_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}