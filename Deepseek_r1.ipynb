{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0MJcSZA+iyLKVSvslJ2pY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Deepseek_r1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed"
      ],
      "metadata": {
        "id": "9O_ylKGhyJoM",
        "outputId": "6ac29a12-bbeb-484f-d57f-63755b9da330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.3.tar.gz (1.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m1.3/1.4 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.0)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.3-py3-none-any.whl size=1549937 sha256=49539898a00d3d97f9ba7c3868248d46ef8aeb63f64ad509b3d3807a92e6cec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/dc/d4/7e7e07b11bc7c0e2a1a495b967acf58de61261eed4596fb23b\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.3 hjson-3.1.0 ninja-1.11.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOeZaV5HxcP_",
        "outputId": "1b854a33-4ce6-4bc4-bbd0-4f57fe724471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-01-28 16:40:24,363] [WARNING] [real_accelerator.py:181:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
            "[2025-01-28 16:40:24,368] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import deepspeed\n",
        "\n",
        "# Define the grid environment\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.state = (0, 0)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        if action == 0 and x > 0:  # Up\n",
        "            x -= 1\n",
        "        elif action == 1 and x < self.size - 1:  # Down\n",
        "            x += 1\n",
        "        elif action == 2 and y > 0:  # Left\n",
        "            y -= 1\n",
        "        elif action == 3 and y < self.size - 1:  # Right\n",
        "            y += 1\n",
        "\n",
        "        self.state = (x, y)\n",
        "        reward = 1 if self.state == self.goal else -0.1\n",
        "        done = self.state == self.goal\n",
        "        return self.state, reward, done\n",
        "\n",
        "# Define the DQN model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Hyperparameters\n",
        "GRID_SIZE = 5\n",
        "STATE_DIM = 2\n",
        "ACTION_DIM = 4\n",
        "EPISODES = 500\n",
        "GAMMA = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_DECAY = 0.995\n",
        "MIN_EPSILON = 0.1\n",
        "BATCH_SIZE = 32\n",
        "MEMORY_SIZE = 10000\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = []\n",
        "        self.size = size\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.buffer) >= self.size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "# Initialize environment, model, and buffer\n",
        "env = GridEnvironment(GRID_SIZE)\n",
        "model = DQN(STATE_DIM, ACTION_DIM)\n",
        "buffer = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "# DeepSpeed configuration\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"Adam\",\n",
        "        \"params\": {\n",
        "            \"lr\": LEARNING_RATE\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize DeepSpeed\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(model=model, model_parameters=model.parameters(), config=deepspeed_config)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    state = torch.FloatTensor(state).unsqueeze(0).to(model_engine.device)\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        if np.random.rand() < EPSILON:\n",
        "            action = np.random.randint(ACTION_DIM)\n",
        "        else:\n",
        "            q_values = model_engine(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(model_engine.device)\n",
        "        buffer.add((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(buffer.buffer) >= BATCH_SIZE:\n",
        "            batch = buffer.sample(BATCH_SIZE)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.cat(states).to(model_engine.device)\n",
        "            actions = torch.LongTensor(actions).unsqueeze(1).to(model_engine.device)\n",
        "            rewards = torch.FloatTensor(rewards).unsqueeze(1).to(model_engine.device)\n",
        "            next_states = torch.cat(next_states).to(model_engine.device)\n",
        "            dones = torch.FloatTensor(dones).unsqueeze(1).to(model_engine.device)\n",
        "\n",
        "            q_values = model_engine(states).gather(1, actions)\n",
        "            next_q_values = model_engine(next_states).max(1)[0].unsqueeze(1)\n",
        "            target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
        "\n",
        "            loss = nn.MSELoss()(q_values, target_q_values)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    EPSILON = max(EPSILON * EPSILON_DECAY, MIN_EPSILON)\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {EPSILON:.2f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ei33S3x3xj_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}