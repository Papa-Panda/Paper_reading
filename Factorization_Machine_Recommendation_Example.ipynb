{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Factorization_Machine_Recommendation_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# --- 0. Dataset Download Function ---\n",
        "def download_and_extract_movielens(url=\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", target_dir=\"ml-100k\"):\n",
        "    \"\"\"\n",
        "    Downloads the MovieLens 100K dataset and extracts it to a specified directory.\n",
        "    \"\"\"\n",
        "    if os.path.exists(target_dir):\n",
        "        print(f\"Dataset directory '{target_dir}' already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading MovieLens 100K dataset from {url}...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "            z.extractall(target_dir)\n",
        "        print(f\"Dataset downloaded and extracted to '{target_dir}'.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        print(\"Please check your internet connection or the URL.\")\n",
        "        exit()\n",
        "    except zipfile.BadZipFile as e:\n",
        "        print(f\"Error extracting zip file: {e}\")\n",
        "        print(\"The downloaded file might be corrupted.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        exit()\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "\n",
        "# Automatically download and extract the dataset if it's not present\n",
        "download_and_extract_movielens()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory 'ml-100k' already exists. Skipping download.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-1Vz1f5Jrt",
        "outputId": "741f06bd-93a8-4fba-8e15-f559c143537c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls ml-100k/ml-100k\n",
        "# # os.path.exists"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK2WfTYP5cpP",
        "outputId": "49677a01-f59b-4a40-8637-dcc12c383d80"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base\tu.genre  u.occupation\n",
            "mku.sh\t   u1.test  u3.base  u4.test  ua.base  ub.test\tu.info\t u.user\n",
            "README\t   u2.base  u3.test  u5.base  ua.test  u.data\tu.item\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to dataset files\n",
        "ratings_path = 'ml-100k/ml-100k/u.data'\n",
        "movies_path = 'ml-100k/ml-100k/u.item'\n",
        "users_path = 'ml-100k/ml-100k/u.user' # Although not explicitly used for features in this FM, useful for context\n",
        "\n",
        "# Check if dataset files exist after attempted download\n",
        "if not os.path.exists(ratings_path) or not os.path.exists(movies_path):\n",
        "    print(\"Dataset files still not found after attempting download. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Load ratings data: user_id | movie_id | rating | timestamp\n",
        "ratings_df = pd.read_csv(ratings_path, sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "\n",
        "# Load movie data: movie_id | movie_title | release_date | video_release_date | IMDb_URL | genre_flags (19 binary)\n",
        "movie_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url'] + \\\n",
        "             ['genre_' + str(i) for i in range(19)]\n",
        "movies_df = pd.read_csv(movies_path, sep='|', names=movie_cols, encoding='latin-1')\n"
      ],
      "metadata": {
        "id": "PZ3KEIRV5RgB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Feature Engineering: Create a Unified Feature Space ---\n",
        "# Factorization Machines require all features (user, item, genre) to be mapped to a single\n",
        "# continuous index space. We'll also maintain \"field\" information for features.\n",
        "\n",
        "# Get unique values for each feature type\n",
        "unique_users = ratings_df['user_id'].unique()\n",
        "unique_movies = ratings_df['movie_id'].unique()\n",
        "genre_cols = ['genre_' + str(i) for i in range(19)]\n",
        "\n",
        "# Create mappings from original IDs/names to a contiguous integer space\n",
        "feature_counter = 0\n",
        "feature_map = {} # Maps (feature_type, original_id) -> global_feature_index\n",
        "field_map = {}   # Maps global_feature_index -> field_id (e.g., user, movie, genre)\n",
        "field_counter = 0\n",
        "\n",
        "# Add User Features\n",
        "user_field_id = field_counter\n",
        "field_counter += 1\n",
        "for user_id in unique_users:\n",
        "    feature_map[('user', user_id)] = feature_counter\n",
        "    field_map[feature_counter] = user_field_id\n",
        "    feature_counter += 1\n",
        "\n",
        "# Add Movie Features\n",
        "movie_field_id = field_counter\n",
        "field_counter += 1\n",
        "for movie_id in unique_movies:\n",
        "    feature_map[('movie', movie_id)] = feature_counter\n",
        "    field_map[feature_counter] = movie_field_id\n",
        "    feature_counter += 1\n",
        "\n",
        "# Add Genre Features\n",
        "genre_field_id = field_counter\n",
        "field_counter += 1\n",
        "for genre_col in genre_cols:\n",
        "    # We only need to map genres that actually exist (are \"1\" for some movie)\n",
        "    # The genre columns in movies_df are already binary (0 or 1)\n",
        "    feature_map[('genre', genre_col)] = feature_counter\n",
        "    field_map[feature_counter] = genre_field_id\n",
        "    feature_counter += 1\n",
        "\n",
        "total_features = feature_counter\n",
        "total_fields = field_counter\n",
        "\n",
        "print(f\"Total unique features (user, movie, genre): {total_features}\")\n",
        "print(f\"Total fields: {total_fields}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cs29LJ5ORi",
        "outputId": "3e02551f-25cf-499e-9eb1-b1a1385b21cb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique features (user, movie, genre): 2644\n",
            "Total fields: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Prepare Training and Test Data for FM ---\n",
        "# For each rating, we construct a sparse feature vector (indices of non-zero features)\n",
        "# and its corresponding target rating.\n",
        "\n",
        "# Split ratings into training and testing sets\n",
        "train_ratings, test_ratings = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
        "\n",
        "def prepare_fm_data(df, movies_df, feature_map, genre_cols):\n",
        "    \"\"\"\n",
        "    Converts a DataFrame of ratings into FM-compatible sparse data.\n",
        "    Returns:\n",
        "        X_indices: List of lists, where each inner list contains the indices of active features for a sample.\n",
        "        y: List of target ratings.\n",
        "    \"\"\"\n",
        "    X_indices = []\n",
        "    y = []\n",
        "    for _, row in df.iterrows():\n",
        "        user_id = row['user_id']\n",
        "        movie_id = row['movie_id']\n",
        "        rating = row['rating']\n",
        "\n",
        "        # Get the global feature index for user and movie\n",
        "        user_feature_idx = feature_map[('user', user_id)]\n",
        "        movie_feature_idx = feature_map[('movie', movie_id)]\n",
        "\n",
        "        # Get genre feature indices for this movie\n",
        "        movie_genres = movies_df[movies_df['movie_id'] == movie_id][genre_cols].iloc[0]\n",
        "        genre_feature_indices = [feature_map[('genre', col)] for col, val in movie_genres.items() if val == 1]\n",
        "\n",
        "        # Combine all active feature indices for this sample\n",
        "        sample_indices = [user_feature_idx, movie_feature_idx] + genre_feature_indices\n",
        "        X_indices.append(sample_indices)\n",
        "        y.append(rating)\n",
        "    return X_indices, np.array(y)\n",
        "\n",
        "print(\"\\nPreparing training data...\")\n",
        "X_train_indices, y_train = prepare_fm_data(train_ratings, movies_df, feature_map, genre_cols)\n",
        "print(\"Preparing test data...\")\n",
        "X_test_indices, y_test = prepare_fm_data(test_ratings, movies_df, feature_map, genre_cols)\n",
        "\n",
        "# --- 4. Factorization Machine Model Implementation ---\n",
        "\n",
        "class FactorizationMachine:\n",
        "    def __init__(self, num_features, k_factors, learning_rate=0.01, reg_w=0.01, reg_v=0.01):\n",
        "        \"\"\"\n",
        "        Initializes the Factorization Machine model.\n",
        "        Args:\n",
        "            num_features (int): Total number of unique features.\n",
        "            k_factors (int): Dimensionality of the latent factors.\n",
        "            learning_rate (float): Learning rate for SGD.\n",
        "            reg_w (float): Regularization strength for linear weights (w).\n",
        "            reg_v (float): Regularization strength for latent vectors (V).\n",
        "        \"\"\"\n",
        "        self.num_features = num_features\n",
        "        self.k_factors = k_factors\n",
        "        self.lr = learning_rate\n",
        "        self.reg_w = reg_w\n",
        "        self.reg_v = reg_v\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.w0 = 0.0  # Global bias\n",
        "        self.w = np.zeros(num_features) # Linear weights\n",
        "        self.V = np.random.normal(0, 0.1, (num_features, k_factors)) # Latent factor matrix\n",
        "\n",
        "    def predict(self, x_indices):\n",
        "        \"\"\"\n",
        "        Calculates the prediction for a single sample (sparse representation).\n",
        "        Args:\n",
        "            x_indices (list): List of indices of active features (where x_i = 1).\n",
        "        Returns:\n",
        "            float: Predicted rating.\n",
        "        \"\"\"\n",
        "        # Linear term: sum(wi * xi)\n",
        "        linear_term = np.sum(self.w[idx] for idx in x_indices)\n",
        "\n",
        "        # Interaction term: 0.5 * sum_f( (sum_i(V_if * xi))^2 - sum_i(V_if^2 * xi^2) )\n",
        "        # Since xi is 1 for active features and 0 otherwise, this simplifies:\n",
        "        interaction_term = 0.0\n",
        "        for f in range(self.k_factors):\n",
        "            # Sum_i(V_if * xi) becomes sum_i(V_if) for active features\n",
        "            sum_Vif_xi = np.sum(self.V[idx, f] for idx in x_indices)\n",
        "            # Sum_i(V_if^2 * xi^2) becomes sum_i(V_if^2) for active features\n",
        "            sum_Vif_sq_xi_sq = np.sum(self.V[idx, f]**2 for idx in x_indices)\n",
        "            interaction_term += (sum_Vif_xi**2 - sum_Vif_sq_xi_sq)\n",
        "        interaction_term *= 0.5\n",
        "\n",
        "        return self.w0 + linear_term + interaction_term\n",
        "\n",
        "    def train(self, X_indices, y, n_epochs=50):\n",
        "        \"\"\"\n",
        "        Trains the FM model using Stochastic Gradient Descent.\n",
        "        Args:\n",
        "            X_indices (list): List of lists, where each inner list contains indices of active features.\n",
        "            y (np.array): Array of target ratings.\n",
        "            n_epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        print(f\"\\nTraining Factorization Machine for {n_epochs} epochs...\")\n",
        "        for epoch in range(n_epochs):\n",
        "            total_loss = 0.0\n",
        "            for i, x_sample_indices in enumerate(X_indices):\n",
        "                actual_rating = y[i]\n",
        "                predicted_rating = self.predict(x_sample_indices)\n",
        "\n",
        "                # Calculate error\n",
        "                error = predicted_rating - actual_rating # We minimize (predicted - actual)^2\n",
        "\n",
        "                # Update global bias (w0)\n",
        "                self.w0 -= self.lr * error\n",
        "\n",
        "                # Update linear weights (w_i)\n",
        "                for idx in x_sample_indices:\n",
        "                    self.w[idx] -= self.lr * (error + self.reg_w * self.w[idx]) # Add regularization\n",
        "\n",
        "                # Update latent vectors (V_if)\n",
        "                for f in range(self.k_factors):\n",
        "                    # Cache sum_Vif_xi for this feature dimension 'f'\n",
        "                    sum_Vif_xi = np.sum(self.V[idx, f] for idx in x_sample_indices)\n",
        "\n",
        "                    for idx in x_sample_indices:\n",
        "                        # Gradient for V_if: error * (sum_Vif_xi - V_if)\n",
        "                        # We multiply by 1.0 here because x_i is 1 for active features\n",
        "                        grad_Vif = error * (sum_Vif_xi - self.V[idx, f]) + self.reg_v * self.V[idx, f]\n",
        "                        self.V[idx, f] -= self.lr * grad_Vif\n",
        "\n",
        "                total_loss += error**2 # Sum of squared errors for MSE\n",
        "\n",
        "            mse = total_loss / len(X_indices)\n",
        "            rmse = sqrt(mse)\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}, Train RMSE: {rmse:.4f}\")\n",
        "\n",
        "# --- 5. Model Training and Evaluation ---\n",
        "\n",
        "k_factors = 10 # Dimensionality of latent factors\n",
        "fm_model = FactorizationMachine(total_features, k_factors, learning_rate=0.01, reg_w=0.01, reg_v=0.01)\n",
        "\n",
        "# Train the model\n",
        "fm_model.train(X_train_indices, y_train, n_epochs=50)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_predictions = []\n",
        "for x_sample_indices in X_test_indices:\n",
        "    predicted_rating = fm_model.predict(x_sample_indices)\n",
        "    # Clip predictions to be within the rating scale (1 to 5)\n",
        "    predicted_rating = max(1, min(5, predicted_rating))\n",
        "    test_predictions.append(predicted_rating)\n",
        "\n",
        "final_rmse = sqrt(mean_squared_error(y_test, test_predictions))\n",
        "print(f\"Final Test RMSE: {final_rmse:.4f}\")\n",
        "\n",
        "# --- 6. Generate Recommendations for a Specific User ---\n",
        "\n",
        "def get_top_n_recommendations(user_id_to_recommend, fm_model, movies_df, feature_map, genre_cols, n=10):\n",
        "    \"\"\"\n",
        "    Generates top N movie recommendations for a given user.\n",
        "    Args:\n",
        "        user_id_to_recommend (int): The ID of the user for whom to generate recommendations.\n",
        "        fm_model (FactorizationMachine): The trained FM model.\n",
        "        movies_df (pd.DataFrame): DataFrame containing movie information.\n",
        "        feature_map (dict): Mapping from (feature_type, original_id) to global_feature_index.\n",
        "        genre_cols (list): List of genre column names.\n",
        "        n (int): Number of top recommendations to return.\n",
        "    Returns:\n",
        "        list: A list of dictionaries, each containing movie_id, title, and predicted_rating.\n",
        "    \"\"\"\n",
        "    # Get movies the user has already rated\n",
        "    rated_movie_ids = ratings_df[ratings_df['user_id'] == user_id_to_recommend]['movie_id'].tolist()\n",
        "\n",
        "    # Get all movie IDs\n",
        "    all_movie_ids = movies_df['movie_id'].tolist()\n",
        "\n",
        "    # Filter out movies the user has already rated\n",
        "    unrated_movie_ids = [m_id for m_id in all_movie_ids if m_id not in rated_movie_ids]\n",
        "\n",
        "    predicted_ratings_for_unrated = []\n",
        "\n",
        "    # Get the user's global feature index once\n",
        "    user_feature_idx = feature_map.get(('user', user_id_to_recommend))\n",
        "    if user_feature_idx is None:\n",
        "        print(f\"Warning: User ID {user_id_to_recommend} not found in feature map.\")\n",
        "        return []\n",
        "\n",
        "    for movie_id in unrated_movie_ids:\n",
        "        # Construct the sparse feature vector for this (user, movie) pair\n",
        "        # This involves the user_id, movie_id, and movie's genres\n",
        "\n",
        "        movie_feature_idx = feature_map.get(('movie', movie_id))\n",
        "        if movie_feature_idx is None:\n",
        "            # Should not happen if all_movie_ids come from movies_df\n",
        "            continue\n",
        "\n",
        "        movie_genres = movies_df[movies_df['movie_id'] == movie_id][genre_cols].iloc[0]\n",
        "        genre_feature_indices = [feature_map[('genre', col)] for col, val in movie_genres.items() if val == 1]\n",
        "\n",
        "        sample_indices = [user_feature_idx, movie_feature_idx] + genre_feature_indices\n",
        "\n",
        "        # Predict rating\n",
        "        predicted_rating = fm_model.predict(sample_indices)\n",
        "        predicted_rating = max(1, min(5, predicted_rating)) # Clip\n",
        "        predicted_ratings_for_unrated.append((movie_id, predicted_rating))\n",
        "\n",
        "    # Sort by predicted rating in descending order\n",
        "    predicted_ratings_for_unrated.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_n_movies = []\n",
        "    for movie_id, rating in predicted_ratings_for_unrated[:n]:\n",
        "        movie_title = movies_df[movies_df['movie_id'] == movie_id]['title'].iloc[0]\n",
        "        top_n_movies.append({'movie_id': movie_id, 'title': movie_title, 'predicted_rating': rating})\n",
        "\n",
        "    return top_n_movies\n",
        "\n",
        "# Example: Get top 5 recommendations for User ID 10\n",
        "user_id_example = 10\n",
        "recommendations = get_top_n_recommendations(user_id_example, fm_model, movies_df, feature_map, genre_cols, n=5)\n",
        "\n",
        "print(f\"\\nTop 5 movie recommendations for User ID {user_id_example}:\")\n",
        "if recommendations:\n",
        "    for rec in recommendations:\n",
        "        print(f\"  - {rec['title']} (Predicted Rating: {rec['predicted_rating']:.2f})\")\n",
        "else:\n",
        "    print(\"  No recommendations found for this user (perhaps user not in dataset or no unrated movies).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZpLftqa5Vof",
        "outputId": "784de9de-6091-4d0a-b587-3893b18d531b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing training data...\n",
            "Preparing test data...\n",
            "\n",
            "Training Factorization Machine for 50 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-fcb6a3f6ea2d>:74: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  linear_term = np.sum(self.w[idx] for idx in x_indices)\n",
            "<ipython-input-22-fcb6a3f6ea2d>:81: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  sum_Vif_xi = np.sum(self.V[idx, f] for idx in x_indices)\n",
            "<ipython-input-22-fcb6a3f6ea2d>:83: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  sum_Vif_sq_xi_sq = np.sum(self.V[idx, f]**2 for idx in x_indices)\n",
            "<ipython-input-22-fcb6a3f6ea2d>:117: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  sum_Vif_xi = np.sum(self.V[idx, f] for idx in x_sample_indices)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train RMSE: 1.0149\n",
            "Epoch 2/50, Train RMSE: 0.9505\n",
            "Epoch 3/50, Train RMSE: 0.9303\n",
            "Epoch 4/50, Train RMSE: 0.9164\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}