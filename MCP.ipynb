{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaduvFekHON8ie/20Dm1ro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/MCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.bilibili.com/video/BV11MEHzwEp6/?spm_id_from=333.337.search-card.all.click&vd_source=83baba81780fd95e96c22e9346057527"
      ],
      "metadata": {
        "id": "2ui5Lf7lI540"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G3nF5zWIqUU",
        "outputId": "2ae2dde4-6ff6-4120-b50e-71eaf34350ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated value for state 'greeting': 1.00\n",
            "Estimated value for state 'math_question': 1.00\n",
            "Estimated value for state 'story_prompt': 1.00\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Environment: Simulated LLM response environment\n",
        "class LLMEnvironment:\n",
        "    def __init__(self):\n",
        "        self.states = [\"greeting\", \"math_question\", \"story_prompt\"]\n",
        "        self.actions = [\"formal\", \"casual\", \"concise\", \"detailed\"]\n",
        "\n",
        "    def get_possible_states(self):\n",
        "        return self.states\n",
        "\n",
        "    def reset(self):\n",
        "        return random.choice(self.states)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        \"\"\"Simulate an LLM response and return a reward\"\"\"\n",
        "        # Toy reward logic\n",
        "        if state == \"greeting\" and action == \"casual\":\n",
        "            reward = 1.0\n",
        "        elif state == \"math_question\" and action == \"concise\":\n",
        "            reward = 1.0\n",
        "        elif state == \"story_prompt\" and action == \"detailed\":\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.2\n",
        "        return reward\n",
        "\n",
        "# Monte Carlo Prediction\n",
        "class MonteCarloPredictor:\n",
        "    def __init__(self, env, policy, episodes=1000):\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.episodes = episodes\n",
        "        self.returns_sum = defaultdict(float)\n",
        "        self.returns_count = defaultdict(int)\n",
        "        self.V = defaultdict(float)\n",
        "\n",
        "    def generate_episode(self):\n",
        "        state = self.env.reset()\n",
        "        action = self.policy(state)\n",
        "        reward = self.env.step(state, action)\n",
        "        return [(state, reward)]\n",
        "\n",
        "    def evaluate(self):\n",
        "        for _ in range(self.episodes):\n",
        "            episode = self.generate_episode()\n",
        "            G = 0.0\n",
        "            visited_states = set()\n",
        "            for (state, reward) in reversed(episode):\n",
        "                G += reward\n",
        "                if state not in visited_states:\n",
        "                    self.returns_sum[state] += G\n",
        "                    self.returns_count[state] += 1\n",
        "                    self.V[state] = self.returns_sum[state] / self.returns_count[state]\n",
        "                    visited_states.add(state)\n",
        "        return self.V\n",
        "\n",
        "# Example policy\n",
        "def simple_policy(state):\n",
        "    return {\n",
        "        \"greeting\": \"casual\",\n",
        "        \"math_question\": \"concise\",\n",
        "        \"story_prompt\": \"detailed\"\n",
        "    }.get(state, \"formal\")\n",
        "\n",
        "# Run\n",
        "env = LLMEnvironment()\n",
        "mcp = MonteCarloPredictor(env, simple_policy, episodes=500)\n",
        "value_estimates = mcp.evaluate()\n",
        "\n",
        "# Show results\n",
        "for state in env.get_possible_states():\n",
        "    print(f\"Estimated value for state '{state}': {value_estimates[state]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85UZiDWOIuNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}