{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/DDPM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amMtw-luK4wd"
      },
      "outputs": [],
      "source": [
        "# generated by https://gemini.google.com/app/090227b96a8ccfd9\n",
        "\n",
        "# inspired by Su Jianlin's series: https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/9119"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K9sssD9KQfr",
        "outputId": "8a97b81c-488b-4cb9-d269-0fb9520dd3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 209kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.90MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.8MB/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Configuration and Hyperparameters ---\n",
        "class Config:\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 25  # Increased epochs for better results\n",
        "    LR = 3e-4\n",
        "    TIMESTEPS = 1000  # Number of steps in the diffusion process\n",
        "    IMG_SIZE = 32  # Resize images to 32x32 for faster training\n",
        "\n",
        "    # Create a directory to save sampled images\n",
        "    OUTPUT_DIR = \"ddpm_samples\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "# We normalize images to the range [-1, 1], which is a common practice for diffusion models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "# --- 2. Diffusion Scheduler (The \"Forward Process\") ---\n",
        "# The forward process gradually adds noise to an image according to a variance schedule.\n",
        "# This part is fixed and does not involve any learnable parameters.\n",
        "\n",
        "def get_linear_schedule(timesteps):\n",
        "    \"\"\"\n",
        "    Creates a linear variance schedule for the diffusion process.\n",
        "    beta_t determines how much noise is added at timestep t.\n",
        "    \"\"\"\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps, device=cfg.DEVICE)\n",
        "\n",
        "betas = get_linear_schedule(cfg.TIMESTEPS)\n",
        "alphas = 1. - betas\n",
        "alphas_hat = torch.cumprod(alphas, dim=0) # Cumulative product: α_hat_t = Π_{i=1 to t} α_i\n",
        "\n",
        "# Pre-calculate values needed for the forward process formula\n",
        "sqrt_alphas_hat = torch.sqrt(alphas_hat)\n",
        "sqrt_one_minus_alphas_hat = torch.sqrt(1. - alphas_hat)\n",
        "\n",
        "def forward_diffusion(x0, t):\n",
        "    \"\"\"\n",
        "    Takes a batch of clean images (x0) and a batch of timesteps (t),\n",
        "    and returns the corresponding noised images (xt) and the noise that was added.\n",
        "\n",
        "    Formula: xt = sqrt(α_hat_t) * x0 + sqrt(1 - α_hat_t) * ε\n",
        "    where ε is random noise.\n",
        "    \"\"\"\n",
        "    # Sample random noise from a standard normal distribution\n",
        "    noise = torch.randn_like(x0)\n",
        "\n",
        "    # Get the sqrt(alpha_hat) and sqrt(1 - alpha_hat) values for the given timesteps\n",
        "    # .gather() selects the values from the tensors at the indices specified by t\n",
        "    # .reshape() is used to make the tensors broadcastable with the image batch\n",
        "    sqrt_alphas_hat_t = sqrt_alphas_hat.gather(-1, t).reshape(cfg.BATCH_SIZE, 1, 1, 1)\n",
        "    sqrt_one_minus_alphas_hat_t = sqrt_one_minus_alphas_hat.gather(-1, t).reshape(cfg.BATCH_SIZE, 1, 1, 1)\n",
        "\n",
        "    # Apply the forward process formula to get the noised image\n",
        "    xt = sqrt_alphas_hat_t * x0 + sqrt_one_minus_alphas_hat_t * noise\n",
        "    return xt, noise\n",
        "\n",
        "# --- 3. The Model (A Simple U-Net for Noise Prediction) ---\n",
        "# The model's job is to look at a noisy image xt and predict the noise (ε)\n",
        "# that was added to the original image x0 at timestep t.\n",
        "# A U-Net architecture is well-suited for this task.\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Simplified U-Net structure. A real implementation would use time embeddings and attention.\n",
        "        self.down1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.down2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        self.up_conv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.up1 = nn.Conv2d(128, 64, 3, padding=1) # Note: input channels = 64 (from up_conv) + 64 (from skip)\n",
        "        self.up_conv2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "        self.up2 = nn.Conv2d(64, 32, 3, padding=1)  # Note: input channels = 32 (from up_conv) + 32 (from skip)\n",
        "\n",
        "        self.out = nn.Conv2d(32, 1, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder Path\n",
        "        d1 = self.relu(self.down1(x))\n",
        "        p1 = self.pool(d1)\n",
        "        d2 = self.relu(self.down2(p1))\n",
        "        p2 = self.pool(d2)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.relu(self.bottleneck(p2))\n",
        "\n",
        "        # Decoder Path with Skip Connections\n",
        "        u1 = self.up_conv1(b)\n",
        "        # The skip connection concatenates the output from the corresponding encoder layer\n",
        "        skip1 = torch.cat([u1, d2], dim=1)\n",
        "        u1_out = self.relu(self.up1(skip1))\n",
        "\n",
        "        u2 = self.up_conv2(u1_out)\n",
        "        skip2 = torch.cat([u2, d1], dim=1)\n",
        "        u2_out = self.relu(self.up2(skip2))\n",
        "\n",
        "        return self.out(u2_out)\n",
        "\n",
        "# --- 4. The Training Loop ---\n",
        "model = SimpleUNet().to(cfg.DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.LR)\n",
        "criterion = nn.MSELoss() # The paper uses L1 or L2 loss. MSE (L2) is common.\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(cfg.EPOCHS):\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{cfg.EPOCHS}\")\n",
        "    total_loss = 0\n",
        "    for step, (images, _) in enumerate(pbar):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x0 = images.to(cfg.DEVICE)\n",
        "\n",
        "        # 1. Sample random timesteps `t` for each image in the batch\n",
        "        t = torch.randint(0, cfg.TIMESTEPS, (cfg.BATCH_SIZE,), device=cfg.DEVICE).long()\n",
        "\n",
        "        # 2. Create noisy images `xt` using the forward process and get the added noise\n",
        "        xt, real_noise = forward_diffusion(x0, t)\n",
        "\n",
        "        # 3. The model predicts the noise from the noised image\n",
        "        predicted_noise = model(xt)\n",
        "\n",
        "        # 4. Calculate the loss between the real noise and the predicted noise\n",
        "        loss = criterion(real_noise, predicted_noise)\n",
        "\n",
        "        # 5. Backpropagate and update model weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # --- 5. Sampling (The \"Reverse Process\") after each epoch ---\n",
        "    if (epoch + 1) % 5 == 0: # Sample every 5 epochs\n",
        "        print(f\"Sampling images at epoch {epoch+1}...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Start with pure random noise (this is x_T)\n",
        "            img = torch.randn((64, 1, cfg.IMG_SIZE, cfg.IMG_SIZE), device=cfg.DEVICE)\n",
        "\n",
        "            # Loop backwards from T-1 down to 0\n",
        "            for i in tqdm(reversed(range(cfg.TIMESTEPS)), desc=\"Sampling loop\", total=cfg.TIMESTEPS):\n",
        "                t = torch.full((64,), i, device=cfg.DEVICE, dtype=torch.long)\n",
        "\n",
        "                predicted_noise = model(img)\n",
        "\n",
        "                # Get pre-calculated values for the current timestep\n",
        "                alpha_t = alphas[t][:, None, None, None]\n",
        "                alpha_hat_t = alphas_hat[t][:, None, None, None]\n",
        "                beta_t = betas[t][:, None, None, None]\n",
        "\n",
        "                # The core of the reverse process step\n",
        "                # 1. Calculate the mean of the distribution for x_{t-1}\n",
        "                term1 = 1 / torch.sqrt(alpha_t)\n",
        "                term2 = (1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)\n",
        "                mean = term1 * (img - term2 * predicted_noise)\n",
        "\n",
        "                # 2. Add noise (variance) to get the final x_{t-1}\n",
        "                if i != 0:\n",
        "                    noise = torch.randn_like(img)\n",
        "                    variance = torch.sqrt(beta_t) * noise\n",
        "                    img = mean + variance\n",
        "                else: # No noise at the last step\n",
        "                    img = mean\n",
        "\n",
        "        model.train()\n",
        "        # Denormalize from [-1, 1] to [0, 1] and clamp\n",
        "        img = (img.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "        # Save a grid of the generated images\n",
        "        grid = make_grid(img, nrow=8)\n",
        "        save_path = os.path.join(cfg.OUTPUT_DIR, f\"ddpm_sample_epoch_{epoch+1}.png\")\n",
        "        save_image(grid, save_path)\n",
        "        print(f\"Saved sampled images to {save_path}\")\n",
        "\n",
        "        # --- Plot the generated images ---\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Sampled Images at Epoch {epoch+1}\")\n",
        "        # We need to permute the tensor dimensions from (C, H, W) to (H, W, C) for matplotlib\n",
        "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "        plt.show()\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HCY5qnyQGjP",
        "outputId": "6966cc5f-7623-4d72-ad54-c70bd25f0687"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxoBYrgtQG3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}