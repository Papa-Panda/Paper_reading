{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Deep_Interest_Network_(DIN)_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gemini.google.com/app/72f602f1dcb0baa4\n",
        "# 屠龙少年与龙：漫谈深度学习驱动的广告推荐技术发展周期 - 朱小强的文章 - 知乎\n",
        "# https://zhuanlan.zhihu.com/p/398041971"
      ],
      "metadata": {
        "id": "NVQSI-ivK0Vb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# --- 1. Synthetic Data Generation ---\n",
        "# This dataset simulates user interactions with items.\n",
        "# Each user has a history of clicked items, and we'll predict if they click a new candidate item.\n",
        "\n",
        "def generate_synthetic_data(num_samples=10000, num_users=1000, num_items=500, history_length=10):\n",
        "    \"\"\"\n",
        "    Generates synthetic data for a Deep Interest Network.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Total number of data points (user-candidate pairs).\n",
        "        num_users (int): Number of unique users.\n",
        "        num_items (int): Number of unique items.\n",
        "        history_length (int): Maximum length of a user's historical clicked items.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - np.array: User IDs.\n",
        "            - np.array: Candidate Item IDs.\n",
        "            - np.array: 2D array of Historical Item IDs (padded with 0s).\n",
        "            - np.array: Labels (1 if clicked, 0 otherwise).\n",
        "            - np.array: Item feature embeddings (simulated).\n",
        "    \"\"\"\n",
        "    user_ids = np.random.randint(1, num_users + 1, num_samples) # User IDs start from 1\n",
        "    candidate_item_ids = np.random.randint(1, num_items + 1, num_samples) # Item IDs start from 1\n",
        "\n",
        "    # Simulate historical clicked items for each user\n",
        "    historical_item_ids = []\n",
        "    for _ in range(num_samples):\n",
        "        # Random number of historical items for each sample, up to history_length\n",
        "        current_history_len = np.random.randint(1, history_length + 1)\n",
        "        history = np.random.randint(1, num_items + 1, current_history_len).tolist()\n",
        "        # Pad history with 0s if less than history_length\n",
        "        history.extend([0] * (history_length - len(history)))\n",
        "        historical_item_ids.append(history)\n",
        "    historical_item_ids = np.array(historical_item_ids)\n",
        "\n",
        "    # Simulate labels (e.g., 1 if candidate item is \"similar\" to history, 0 otherwise)\n",
        "    # This is a very simple simulation of a click, based on random chance\n",
        "    labels = np.random.randint(0, 2, num_samples)\n",
        "\n",
        "    # Simulate item feature embeddings (e.g., each item has a 16-dim embedding)\n",
        "    # We will use this for the DIN's attention mechanism\n",
        "    embedding_dim = 16\n",
        "    item_features = np.random.rand(num_items + 1, embedding_dim) # +1 for 0-padding, item 0 is dummy\n",
        "\n",
        "    print(f\"Generated synthetic data: {num_samples} samples.\")\n",
        "    print(f\"  User IDs shape: {user_ids.shape}\")\n",
        "    print(f\"  Candidate Item IDs shape: {candidate_item_ids.shape}\")\n",
        "    print(f\"  Historical Item IDs shape: {historical_item_ids.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  Item Features shape: {item_features.shape}\")\n",
        "\n",
        "    return user_ids, candidate_item_ids, historical_item_ids, labels, item_features\n",
        "\n",
        "# Generate the data\n",
        "num_users = 1000\n",
        "num_items = 500\n",
        "history_length = 10 # Max length of user behavior sequence\n",
        "embedding_dim = 16 # Dimensionality of item and user embeddings\n",
        "\n",
        "user_ids_data, candidate_item_ids_data, historical_item_ids_data, labels_data, item_features_data = \\\n",
        "    generate_synthetic_data(num_samples=50000, num_users=num_users, num_items=num_items, history_length=history_length)\n",
        "\n",
        "# --- 2. DIN Model Architecture ---\n",
        "\n",
        "# Custom Attention Layer for DIN\n",
        "class Dice(layers.Layer):\n",
        "    \"\"\"\n",
        "    Data Adaptive Activation Function (DICE) for DIN.\n",
        "    It's a variant of PReLu that learns a dynamic 'p' parameter.\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.alphas = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='dice_alpha'\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='dice_beta'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate mean and variance along the specified axis\n",
        "        reduc_axis = list(range(len(inputs.shape)))\n",
        "        if self.axis != -1:\n",
        "            reduc_axis.pop(self.axis)\n",
        "        mean = tf.reduce_mean(inputs, axis=reduc_axis, keepdims=True)\n",
        "        variance = tf.reduce_mean(tf.square(inputs - mean), axis=reduc_axis, keepdims=True)\n",
        "\n",
        "        # Normalize the input\n",
        "        x_normed = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Calculate p (the parameter for PReLu-like activation)\n",
        "        p = tf.sigmoid(self.alphas * x_normed + self.beta)\n",
        "\n",
        "        # Apply DICE activation\n",
        "        return p * inputs + (1 - p) * self.alphas * inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"axis\": self.axis,\n",
        "            \"epsilon\": self.epsilon,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class AttentionPoolingLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Attention pooling layer for Deep Interest Network (DIN).\n",
        "    Calculates attention scores between candidate item and historical items,\n",
        "    then weights the historical item embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_units=[80, 40], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "        # Attention network (e.g., MLP)\n",
        "        self.dense_layers = []\n",
        "        for units in hidden_units:\n",
        "            self.dense_layers.append(layers.Dense(units, activation=None)) # No activation initially\n",
        "            self.dense_layers.append(Dice()) # Use Dice activation after each dense layer\n",
        "\n",
        "        self.output_layer = layers.Dense(1, activation=None) # Output attention score (scalar)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: [candidate_item_embedding, historical_item_embeddings]\n",
        "        # candidate_item_embedding: (batch_size, embedding_dim)\n",
        "        # historical_item_embeddings: (batch_size, history_length, embedding_dim)\n",
        "\n",
        "        candidate_item_embedding, historical_item_embeddings = inputs\n",
        "\n",
        "        # Expand candidate_item_embedding to match history_length dimension for concatenation\n",
        "        # (batch_size, 1, embedding_dim) -> (batch_size, history_length, embedding_dim)\n",
        "        candidate_item_embedding_expanded = tf.expand_dims(candidate_item_embedding, 1)\n",
        "        candidate_item_embedding_tiled = tf.tile(candidate_item_embedding_expanded, [1, tf.shape(historical_item_embeddings)[1], 1])\n",
        "\n",
        "        # Concatenate candidate item, historical item, their product, and their difference\n",
        "        # This is a common practice in attention mechanisms for DIN\n",
        "        # (batch_size, history_length, embedding_dim * 4)\n",
        "        concatenated_features = tf.concat([\n",
        "            candidate_item_embedding_tiled,\n",
        "            historical_item_embeddings,\n",
        "            candidate_item_embedding_tiled * historical_item_embeddings,\n",
        "            candidate_item_embedding_tiled - historical_item_embeddings\n",
        "        ], axis=-1)\n",
        "\n",
        "        # Pass through attention network\n",
        "        attention_logits = concatenated_features\n",
        "        for layer in self.dense_layers:\n",
        "            attention_logits = layer(attention_logits)\n",
        "\n",
        "        attention_logits = self.output_layer(attention_logits) # (batch_size, history_length, 1)\n",
        "\n",
        "        # Apply softmax to get attention weights.\n",
        "        # Mask out padded items (where embedding is 0) to prevent them from influencing attention.\n",
        "        # For simplicity, we assume historical_item_embeddings with all zeros corresponds to padding.\n",
        "        # A more robust approach would be to pass a mask explicitly.\n",
        "        mask = tf.cast(tf.reduce_sum(tf.abs(historical_item_embeddings), axis=-1, keepdims=True) > 0, tf.float32)\n",
        "        attention_logits = attention_logits - (1.0 - mask) * 1e9 # Mask padded items with large negative value\n",
        "\n",
        "        attention_weights = tf.nn.softmax(attention_logits, axis=1) # (batch_size, history_length, 1)\n",
        "\n",
        "        # Weighted sum of historical item embeddings\n",
        "        # (batch_size, history_length, embedding_dim) * (batch_size, history_length, 1)\n",
        "        # -> (batch_size, history_length, embedding_dim) -> (batch_size, embedding_dim)\n",
        "        weighted_history_embedding = tf.reduce_sum(attention_weights * historical_item_embeddings, axis=1)\n",
        "\n",
        "        return weighted_history_embedding\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embedding_dim\": self.embedding_dim,\n",
        "            \"hidden_units\": self.hidden_units,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_din_model(num_users, num_items, history_length, embedding_dim, item_features_matrix):\n",
        "    \"\"\"\n",
        "    Builds the Deep Interest Network (DIN) model.\n",
        "\n",
        "    Args:\n",
        "        num_users (int): Total number of unique users.\n",
        "        num_items (int): Total number of unique items.\n",
        "        history_length (int): Maximum length of user historical behavior sequence.\n",
        "        embedding_dim (int): Dimensionality of item and user embeddings.\n",
        "        item_features_matrix (np.array): Pre-trained or initial item feature embeddings.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: Compiled DIN model.\n",
        "    \"\"\"\n",
        "    # Input Layers\n",
        "    user_id_input = keras.Input(shape=(1,), name='user_id_input', dtype='int32')\n",
        "    candidate_item_id_input = keras.Input(shape=(1,), name='candidate_item_id_input', dtype='int32')\n",
        "    historical_item_ids_input = keras.Input(shape=(history_length,), name='historical_item_ids_input', dtype='int32')\n",
        "\n",
        "    # Embedding Layers\n",
        "    # User embeddings: simple lookup\n",
        "    user_embedding_layer = layers.Embedding(\n",
        "        input_dim=num_users + 1, # +1 for 0-padding if user_id 0 exists\n",
        "        output_dim=embedding_dim,\n",
        "        name='user_embedding'\n",
        "    )\n",
        "    user_embedding = user_embedding_layer(user_id_input) # (batch_size, 1, embedding_dim)\n",
        "    user_embedding = layers.Reshape((embedding_dim,))(user_embedding) # (batch_size, embedding_dim)\n",
        "\n",
        "    # Item embeddings: use pre-defined item_features_matrix (e.g., from pre-training or here simulated)\n",
        "    # Set trainable=False if these embeddings are fixed, True if they should be fine-tuned.\n",
        "    item_embedding_layer = layers.Embedding(\n",
        "        input_dim=num_items + 1, # +1 for 0-padding\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[item_features_matrix], # Initialize with the simulated item features\n",
        "        trainable=True, # Allow fine-tuning these embeddings during training\n",
        "        name='item_embedding'\n",
        "    )\n",
        "\n",
        "    # Candidate item embedding\n",
        "    candidate_item_embedding = item_embedding_layer(candidate_item_id_input) # (batch_size, 1, embedding_dim)\n",
        "    candidate_item_embedding = layers.Reshape((embedding_dim,))(candidate_item_embedding) # (batch_size, embedding_dim)\n",
        "\n",
        "    # Historical items embeddings\n",
        "    historical_item_embeddings = item_embedding_layer(historical_item_ids_input) # (batch_size, history_length, embedding_dim)\n",
        "\n",
        "    # DIN Attention Mechanism\n",
        "    # The AttentionPoolingLayer computes a weighted sum of historical item embeddings\n",
        "    # based on their relevance to the candidate item.\n",
        "    attention_output = AttentionPoolingLayer(\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_units=[80, 40], # Attention MLP hidden units\n",
        "        name='din_attention_pooling'\n",
        "    )([candidate_item_embedding, historical_item_embeddings])\n",
        "\n",
        "    # Concatenate all features for the final prediction layer\n",
        "    # These are: user_embedding, candidate_item_embedding, and the attention-weighted historical embedding\n",
        "    concatenated_features = layers.concatenate([\n",
        "        user_embedding,\n",
        "        candidate_item_embedding,\n",
        "        attention_output\n",
        "    ], axis=-1)\n",
        "\n",
        "    # Prediction MLP (Deep Network)\n",
        "    mlp_output = layers.Dense(128, activation='relu')(concatenated_features)\n",
        "    mlp_output = layers.Dropout(0.3)(mlp_output)\n",
        "    mlp_output = layers.Dense(64, activation='relu')(mlp_output)\n",
        "    mlp_output = layers.Dropout(0.3)(mlp_output)\n",
        "    mlp_output = layers.Dense(32, activation='relu')(mlp_output)\n",
        "\n",
        "    # Output layer (sigmoid for binary classification)\n",
        "    output = layers.Dense(1, activation='sigmoid', name='output')(mlp_output)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(\n",
        "        inputs=[user_id_input, candidate_item_id_input, historical_item_ids_input],\n",
        "        outputs=output,\n",
        "        name='deep_interest_network'\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the DIN model\n",
        "din_model = build_din_model(\n",
        "    num_users=num_users,\n",
        "    num_items=num_items,\n",
        "    history_length=history_length,\n",
        "    embedding_dim=embedding_dim,\n",
        "    item_features_matrix=item_features_data\n",
        ")\n",
        "\n",
        "din_model.summary()\n",
        "\n",
        "# --- 3. Prepare Data for Training ---\n",
        "# Create a dictionary for model inputs\n",
        "model_inputs = {\n",
        "    'user_id_input': user_ids_data,\n",
        "    'candidate_item_id_input': candidate_item_ids_data,\n",
        "    'historical_item_ids_input': historical_item_ids_data\n",
        "}\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "print(\"\\n--- Training the DIN Model ---\")\n",
        "history = din_model.fit(\n",
        "    model_inputs,\n",
        "    labels_data,\n",
        "    batch_size=256,\n",
        "    epochs=5, # Using a small number of epochs for demonstration\n",
        "    validation_split=0.2, # Use 20% of data for validation\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Final training AUC: {history.history['auc'][-1]:.4f}\")\n",
        "print(f\"Final validation AUC: {history.history['val_auc'][-1]:.4f}\")\n",
        "\n",
        "# --- 5. Make Predictions (Example) ---\n",
        "print(\"\\n--- Making Predictions (Example) ---\")\n",
        "\n",
        "# Select a few random samples for prediction\n",
        "num_predict_samples = 5\n",
        "random_indices = np.random.choice(len(user_ids_data), num_predict_samples, replace=False)\n",
        "\n",
        "sample_user_ids = user_ids_data[random_indices]\n",
        "sample_candidate_item_ids = candidate_item_ids_data[random_indices]\n",
        "sample_historical_item_ids = historical_item_ids_data[random_indices]\n",
        "sample_labels = labels_data[random_indices]\n",
        "\n",
        "sample_inputs = {\n",
        "    'user_id_input': sample_user_ids,\n",
        "    'candidate_item_id_input': sample_candidate_item_ids,\n",
        "    'historical_item_ids_input': sample_historical_item_ids\n",
        "}\n",
        "\n",
        "predictions = din_model.predict(sample_inputs)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(num_predict_samples):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    User ID: {sample_user_ids[i][0] if sample_user_ids[i].ndim > 0 else sample_user_ids[i]}\")\n",
        "    print(f\"    Candidate Item ID: {sample_candidate_item_"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 347) (<ipython-input-2-064493d31321>, line 347)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-064493d31321>\"\u001b[0;36m, line \u001b[0;32m347\u001b[0m\n\u001b[0;31m    print(f\"    Candidate Item ID: {sample_candidate_item_\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 347)\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "vbMAHU8HKzsw",
        "outputId": "6d0de44f-0acc-4311-885b-016e2f6cdb57"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXwHrD7bK1mA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}