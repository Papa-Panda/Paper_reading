OPtimizer Related
-- 梯度下降和EM算法：系出同源，一脉相承
  https://spaces.ac.cn/archives/4277
-- 训练1000层的Transformer究竟有什么困难？
  https://spaces.ac.cn/archives/8978
  hard to understand for now



GAN related
-- # 互怼的艺术：从零直达WGAN-GP
  # https://spaces.ac.cn/archives/4439
  # some math to do
  implemented in https://colab.research.google.com/drive/103UNcEXBvOXYA7MjRCshbnAbCb9TZCfE#scrollTo=HqP5te5b3r3O

-- 能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”
  https://spaces.ac.cn/archives/6316
  对于GAN来说，最通俗易懂的视角当属“造假者-鉴别者”相互竞争的类比，这个视角直接导致了标准的GAN。但是，这个通俗的类比无法进一步延伸到WGAN乃至梯度惩罚等正则项的理解。
  相比之下，能量视角相当灵活，它甚至能让我们直观地理解WGAN、梯度惩罚等内容，这些内容可以说是目前GAN领域最先进的部分成果了。
  虽然看起来能量视角比“造假者-鉴别者”形式上复杂一些，但其实它的物理意义也相当清晰，稍加思考，我们会感觉到它其实更为有趣、更具有启发性，有种“越嚼越有味”的感觉～
-- # 能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”
# https://www.spaces.ac.cn/archives/6331
# did the math; overlooked the most complicated part
-- # 能量视角下的GAN模型（三）：生成模型=能量模型
# https://www.spaces.ac.cn/archives/6612
-- # 用变分推断统一理解生成模型（VAE、EM GAN、AAE、ALI）
  # https://spaces.ac.cn/archives/5716
  # I have some math to share, except AAE/ALI


自编码器与变分自编码器（VAE）

VQ-VAE的简明介绍：量子化自编码器变分自编码器 = 最小化先验分布 + 最大化互信息变分自编码器
（一）：原来是这么一回事变分自编码器
（二）：从贝叶斯观点出发变分自编码器
（三）：这样做为什么能成？变分自编码器
（四）：一步到位的聚类方案变分自编码器
（五）：VAE + BN = 更好的VAE变分自编码器
（六）：从几何视角来理解VAE的尝试变分自编码器
（七）：球面上的VAE（vMF-VAE）变分自编码器
（八）：估计样本概率密度EAE：自编码器 + BN + 最大熵 = 生成模型
