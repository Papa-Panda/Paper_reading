{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDa+YX1khrW9+oH154lZgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/MoCo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-m6J_bP45ANH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "class MoCo(nn.Module):\n",
        "    def __init__(self, base_encoder, dim=128, K=65536, m=0.999, T=0.07):\n",
        "        super(MoCo, self).__init__()\n",
        "\n",
        "        # Create the query and key encoders\n",
        "        self.encoder_q = base_encoder(num_classes=dim)\n",
        "        self.encoder_k = copy.deepcopy(self.encoder_q)\n",
        "\n",
        "        # Freeze key encoder (updated via momentum)\n",
        "        for param_k in self.encoder_k.parameters():\n",
        "            param_k.requires_grad = False\n",
        "\n",
        "        # MoCo parameters\n",
        "        self.K = K  # Queue size\n",
        "        self.m = m  # Momentum for the key encoder update\n",
        "        self.T = T  # Softmax temperature\n",
        "\n",
        "        # Create the queue (K x dim)\n",
        "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
        "        self.queue = F.normalize(self.queue, dim=0)\n",
        "\n",
        "        # Pointer for queue updates\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        \"\"\"Momentum update of the key encoder\"\"\"\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
        "\n",
        "    def forward(self, im_q, im_k):\n",
        "        \"\"\"Input: im_q (query image), im_k (key image)\"\"\"\n",
        "        # Compute query features\n",
        "        q = self.encoder_q(im_q)  # Queries: (N, dim)\n",
        "        q = F.normalize(q, dim=1)\n",
        "\n",
        "        # Compute key features\n",
        "        with torch.no_grad():\n",
        "            self._momentum_update_key_encoder()  # Update key encoder\n",
        "            k = self.encoder_k(im_k)  # Keys: (N, dim)\n",
        "            k = F.normalize(k, dim=1)\n",
        "\n",
        "        # Compute logits\n",
        "        # Positive logits: Nx1\n",
        "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
        "\n",
        "        # Negative logits: NxK\n",
        "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
        "\n",
        "        # Logits: Nx(1+K)\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "        # Apply temperature\n",
        "        logits /= self.T\n",
        "\n",
        "        # Labels: positive key is at index 0\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)\n",
        "\n",
        "        # Update the queue\n",
        "        self._dequeue_and_enqueue(k)\n",
        "\n",
        "        return logits, labels\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, keys):\n",
        "        \"\"\"Dequeue the oldest keys and enqueue the new ones.\"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0  # Ensure that the batch fits the queue size\n",
        "\n",
        "        # Replace the keys at ptr (dequeue)\n",
        "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
        "\n",
        "        # Move the pointer\n",
        "        ptr = (ptr + batch_size) % self.K\n",
        "        self.queue_ptr[0] = ptr"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3SRjT2D6U_K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 for example usage\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet default input size is 224x224\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Initialize the MoCo model with ResNet50 backbone\n",
        "base_encoder = models.resnet50\n",
        "# # no GPU override\n",
        "# # model = MoCo(base_encoder=base_encoder).cuda()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MoCo(base_encoder=base_encoder).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model.encoder_q.parameters(), lr=0.03, momentum=0.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r88Sucq95iL1",
        "outputId": "2e6f3de2-a10f-43e6-d681-fbed62dd5067"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(10):  # Simple 10 epochs\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        # no GPU switch to cpu\n",
        "        # # Get two random augmentations of the same image\n",
        "        # im_q = images.cuda()\n",
        "        # im_k = images.cuda()  # In practice, you would have another view for im_k\n",
        "        im_q = images.to(device)\n",
        "        im_k = images.to(device)\n",
        "\n",
        "        # Forward pass through MoCo\n",
        "        logits, labels = model(im_q, im_k)\n",
        "\n",
        "        # Cross-entropy loss\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{10}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "2kP8p35T52pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1RmGeom6h8d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}