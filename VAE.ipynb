{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMffyt5jGnV1X7M8KHG9wVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# created by ??? cannot find link"
      ],
      "metadata": {
        "id": "bqurA52Cz3gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-gIOg6mY6878"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define Variational Autoencoder (VAE) Model\n",
        "# updated the sizes for mnist dataset\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder: Convolutional layers to downsample the input\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
        "            # floor((32 - 3 + 1 + 2 * 0)/2) + 1 = 16\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # 16x16 -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # 8x8 -> 4x4\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "\n",
        "        # Latent vectors `mu` and `log_var`\n",
        "        # self.fc_mu = nn.Linear(256*4*4, latent_dim)     # Mean of the latent space\n",
        "        # self.fc_log_var = nn.Linear(256*4*4, latent_dim) # Log-variance of the latent space\n",
        "        self.fc_mu = nn.Linear(256*3*3, latent_dim)     # Mean of the latent space\n",
        "        self.fc_log_var = nn.Linear(256*3*3, latent_dim) # Log-variance of the latent space\n",
        "\n",
        "\n",
        "\n",
        "        # Decoder: Fully connected + transposed convolutions to upsample the latent representation\n",
        "        # self.fc_decoder = nn.Linear(latent_dim, 256*4*4)\n",
        "        self.fc_decoder = nn.Linear(latent_dim, 256*3*3)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # output size (I−1)S−2P+K\n",
        "            # (3 - 1) * 2 - 2 * 1 + 4 = 4\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 8x8 -> 16x16\n",
        "            nn.ReLU(),\n",
        "            # nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # 16x16 -> 32x32\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=6, stride=2, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encodes the input into latent space\"\"\"\n",
        "        x = self.encoder(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the image to a vector\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_log_var(x)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"Sample from a Gaussian distribution using the reparameterization trick\"\"\"\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Decodes the latent variable `z` back to an image\"\"\"\n",
        "        x = self.fc_decoder(z)\n",
        "        # x = x.view(x.size(0), 256, 4, 4)  # Reshape to image size\n",
        "        x = x.view(x.size(0), 256, 3, 3)  # Reshape to image size\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through VAE\"\"\"\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        reconstructed = self.decode(z)\n",
        "        return reconstructed, mu, log_var\n",
        "\n",
        "# Step 2: Define the loss function (Reconstruction loss + KL Divergence)\n",
        "# def vae_loss(reconstructed, original, mu, log_var):\n",
        "#     # Reconstruction loss (binary cross entropy)\n",
        "#     recon_loss = nn.functional.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
        "\n",
        "#     # KL Divergence loss (encourages the latent space to be Gaussian)\n",
        "#     kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "#     return recon_loss + kl_divergence\n",
        "\n",
        "\n",
        "# note the vae loss function is the core why it works\n",
        "# see explanation here: https://spaces.ac.cn/archives/5343\n",
        "def vae_loss(reconstructed, original, mu, log_var):\n",
        "    # Reconstruction loss using Mean Squared Error (MSE)\n",
        "    recon_loss = nn.functional.mse_loss(reconstructed, original, reduction='sum')\n",
        "\n",
        "    # KL Divergence loss (encourages the latent space to be Gaussian)\n",
        "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    return recon_loss + kl_divergence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def load_mnist_data(batch_size=100):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the MNIST dataset using PyTorch's DataLoader.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # Converts to tensor and scales to [0, 1]\n",
        "    ])\n",
        "\n",
        "    # Check if data directory exists, create if not\n",
        "    data_dir = './data'\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "gPhwQvhWnDox"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "trainloader = load_mnist_data(batch_size=64)[0]\n",
        "\n",
        "# Step 4: Initialize VAE model, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VAE(latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWS3Skyv9wOM",
        "outputId": "e77e0cde-6407-4686-b7a2-ca83df304ef8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.10MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.68MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, _ = data\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        reconstructed, mu, log_var = model(inputs)\n",
        "\n",
        "        # Compute VAE loss\n",
        "        loss = vae_loss(reconstructed, inputs, mu, log_var)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 2:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {loss.item()}')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(trainloader)}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Step 6: Sample from the latent space to generate new images\n",
        "def sample_from_latent_space(model, num_samples=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, 128).to(device)  # Sample random latent vectors\n",
        "        generated_images = model.decode(z)\n",
        "        generated_images = generated_images.cpu()\n",
        "\n",
        "    # Plot the generated images\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(generated_images[i].permute(1, 2, 0).numpy())\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Generate new images\n",
        "sample_from_latent_space(model, num_samples=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U9pqZjce94xa",
        "outputId": "202126a0-19dd-446c-ee55-7d973a5e140f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [3/938], Loss: 10118.568359375\n",
            "Epoch [1/10], Step [103/938], Loss: 3822.009033203125\n",
            "Epoch [1/10], Step [203/938], Loss: 3251.144775390625\n",
            "Epoch [1/10], Step [303/938], Loss: 3087.291748046875\n",
            "Epoch [1/10], Step [403/938], Loss: 2604.3916015625\n",
            "Epoch [1/10], Step [503/938], Loss: 2406.27197265625\n",
            "Epoch [1/10], Step [603/938], Loss: 2473.601806640625\n",
            "Epoch [1/10], Step [703/938], Loss: 2544.57958984375\n",
            "Epoch [1/10], Step [803/938], Loss: 2137.339111328125\n",
            "Epoch [1/10], Step [903/938], Loss: 2415.307861328125\n",
            "Epoch [1/10], Loss: 2908.3513157565967\n",
            "Epoch [2/10], Step [3/938], Loss: 2218.801513671875\n",
            "Epoch [2/10], Step [103/938], Loss: 2336.963134765625\n",
            "Epoch [2/10], Step [203/938], Loss: 2164.17529296875\n",
            "Epoch [2/10], Step [303/938], Loss: 2227.83837890625\n",
            "Epoch [2/10], Step [403/938], Loss: 2216.164794921875\n",
            "Epoch [2/10], Step [503/938], Loss: 1965.25927734375\n",
            "Epoch [2/10], Step [603/938], Loss: 2036.0606689453125\n",
            "Epoch [2/10], Step [703/938], Loss: 2062.302978515625\n",
            "Epoch [2/10], Step [803/938], Loss: 2012.43408203125\n",
            "Epoch [2/10], Step [903/938], Loss: 1957.4881591796875\n",
            "Epoch [2/10], Loss: 2090.5001827801216\n",
            "Epoch [3/10], Step [3/938], Loss: 1867.721435546875\n",
            "Epoch [3/10], Step [103/938], Loss: 1927.72998046875\n",
            "Epoch [3/10], Step [203/938], Loss: 1920.01708984375\n",
            "Epoch [3/10], Step [303/938], Loss: 1857.9285888671875\n",
            "Epoch [3/10], Step [403/938], Loss: 2087.6162109375\n",
            "Epoch [3/10], Step [503/938], Loss: 2031.2548828125\n",
            "Epoch [3/10], Step [603/938], Loss: 1890.413818359375\n",
            "Epoch [3/10], Step [703/938], Loss: 1918.4947509765625\n",
            "Epoch [3/10], Step [803/938], Loss: 1942.6737060546875\n",
            "Epoch [3/10], Step [903/938], Loss: 1932.38232421875\n",
            "Epoch [3/10], Loss: 1948.0522088089476\n",
            "Epoch [4/10], Step [3/938], Loss: 1961.1024169921875\n",
            "Epoch [4/10], Step [103/938], Loss: 2009.596435546875\n",
            "Epoch [4/10], Step [203/938], Loss: 1922.805908203125\n",
            "Epoch [4/10], Step [303/938], Loss: 1891.6680908203125\n",
            "Epoch [4/10], Step [403/938], Loss: 1987.697265625\n",
            "Epoch [4/10], Step [503/938], Loss: 1836.2550048828125\n",
            "Epoch [4/10], Step [603/938], Loss: 2039.783935546875\n",
            "Epoch [4/10], Step [703/938], Loss: 1928.53857421875\n",
            "Epoch [4/10], Step [803/938], Loss: 2019.111572265625\n",
            "Epoch [4/10], Step [903/938], Loss: 1950.931396484375\n",
            "Epoch [4/10], Loss: 1896.0395112190165\n",
            "Epoch [5/10], Step [3/938], Loss: 1771.5908203125\n",
            "Epoch [5/10], Step [103/938], Loss: 1926.72509765625\n",
            "Epoch [5/10], Step [203/938], Loss: 1851.1612548828125\n",
            "Epoch [5/10], Step [303/938], Loss: 1960.73828125\n",
            "Epoch [5/10], Step [403/938], Loss: 1834.58349609375\n",
            "Epoch [5/10], Step [503/938], Loss: 1812.6024169921875\n",
            "Epoch [5/10], Step [603/938], Loss: 1827.5830078125\n",
            "Epoch [5/10], Step [703/938], Loss: 1951.017822265625\n",
            "Epoch [5/10], Step [803/938], Loss: 1927.841552734375\n",
            "Epoch [5/10], Step [903/938], Loss: 1858.185546875\n",
            "Epoch [5/10], Loss: 1866.1353649798232\n",
            "Epoch [6/10], Step [3/938], Loss: 1832.941162109375\n",
            "Epoch [6/10], Step [103/938], Loss: 1866.1337890625\n",
            "Epoch [6/10], Step [203/938], Loss: 1743.969970703125\n",
            "Epoch [6/10], Step [303/938], Loss: 1795.1058349609375\n",
            "Epoch [6/10], Step [403/938], Loss: 1846.529541015625\n",
            "Epoch [6/10], Step [503/938], Loss: 1834.53271484375\n",
            "Epoch [6/10], Step [603/938], Loss: 1782.2708740234375\n",
            "Epoch [6/10], Step [703/938], Loss: 1990.6578369140625\n",
            "Epoch [6/10], Step [803/938], Loss: 1807.65234375\n",
            "Epoch [6/10], Step [903/938], Loss: 1807.188720703125\n",
            "Epoch [6/10], Loss: 1847.7474841542844\n",
            "Epoch [7/10], Step [3/938], Loss: 1745.1622314453125\n",
            "Epoch [7/10], Step [103/938], Loss: 1757.865478515625\n",
            "Epoch [7/10], Step [203/938], Loss: 1893.694091796875\n",
            "Epoch [7/10], Step [303/938], Loss: 1807.69140625\n",
            "Epoch [7/10], Step [403/938], Loss: 1800.9954833984375\n",
            "Epoch [7/10], Step [503/938], Loss: 1788.5126953125\n",
            "Epoch [7/10], Step [603/938], Loss: 1721.25537109375\n",
            "Epoch [7/10], Step [703/938], Loss: 1830.1453857421875\n",
            "Epoch [7/10], Step [803/938], Loss: 1773.0093994140625\n",
            "Epoch [7/10], Step [903/938], Loss: 1763.713134765625\n",
            "Epoch [7/10], Loss: 1829.8473947358284\n",
            "Epoch [8/10], Step [3/938], Loss: 1754.005615234375\n",
            "Epoch [8/10], Step [103/938], Loss: 1755.01025390625\n",
            "Epoch [8/10], Step [203/938], Loss: 1847.895263671875\n",
            "Epoch [8/10], Step [303/938], Loss: 1886.357666015625\n",
            "Epoch [8/10], Step [403/938], Loss: 1899.000244140625\n",
            "Epoch [8/10], Step [503/938], Loss: 1874.712646484375\n",
            "Epoch [8/10], Step [603/938], Loss: 1752.11328125\n",
            "Epoch [8/10], Step [703/938], Loss: 1803.76123046875\n",
            "Epoch [8/10], Step [803/938], Loss: 1786.7445068359375\n",
            "Epoch [8/10], Step [903/938], Loss: 1882.75048828125\n",
            "Epoch [8/10], Loss: 1818.3235978571845\n",
            "Epoch [9/10], Step [3/938], Loss: 1866.072021484375\n",
            "Epoch [9/10], Step [103/938], Loss: 1838.9652099609375\n",
            "Epoch [9/10], Step [203/938], Loss: 1738.4423828125\n",
            "Epoch [9/10], Step [303/938], Loss: 1933.79833984375\n",
            "Epoch [9/10], Step [403/938], Loss: 1803.3201904296875\n",
            "Epoch [9/10], Step [503/938], Loss: 1698.5150146484375\n",
            "Epoch [9/10], Step [603/938], Loss: 1837.3114013671875\n",
            "Epoch [9/10], Step [703/938], Loss: 1861.494873046875\n",
            "Epoch [9/10], Step [803/938], Loss: 1817.6942138671875\n",
            "Epoch [9/10], Step [903/938], Loss: 1874.054443359375\n",
            "Epoch [9/10], Loss: 1809.6402796112907\n",
            "Epoch [10/10], Step [3/938], Loss: 1744.931396484375\n",
            "Epoch [10/10], Step [103/938], Loss: 1863.4853515625\n",
            "Epoch [10/10], Step [203/938], Loss: 1845.5985107421875\n",
            "Epoch [10/10], Step [303/938], Loss: 1858.2451171875\n",
            "Epoch [10/10], Step [403/938], Loss: 1719.065185546875\n",
            "Epoch [10/10], Step [503/938], Loss: 1792.13525390625\n",
            "Epoch [10/10], Step [603/938], Loss: 1732.927978515625\n",
            "Epoch [10/10], Step [703/938], Loss: 1764.638916015625\n",
            "Epoch [10/10], Step [803/938], Loss: 1815.2015380859375\n",
            "Epoch [10/10], Step [903/938], Loss: 1854.4769287109375\n",
            "Epoch [10/10], Loss: 1802.5668672020756\n",
            "Finished Training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uT--yAxe_D_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}