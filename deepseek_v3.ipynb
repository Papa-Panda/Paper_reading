{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzkCFKY53Hcakz6O6eAXOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/deepseek_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.bilibili.com/video/BV1uUPieDEK1/?spm_id_from=333.788.videopod.sections&vd_source=1fecee762931e992c96e5e166be13b76\n",
        "# https://chatgpt.com/c/67f3f548-7238-800e-9856-a834f9003957"
      ],
      "metadata": {
        "id": "auh4Gkjb-L2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK7uRlFX98kv",
        "outputId": "13169ab0-613d-4f1b-988c-fa903eb9a631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128, 1000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Rotary positional embeddings\n",
        "def apply_rotary_pos_emb(x, sin, cos):\n",
        "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "    x_rot = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
        "    return x_rot\n",
        "\n",
        "def get_rotary_emb(seq_len, dim, device):\n",
        "    freqs = torch.pow(10000, -torch.arange(0, dim, 2).float() / dim).to(device)\n",
        "    t = torch.arange(seq_len, device=device).float()\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    return torch.sin(freqs), torch.cos(freqs)\n",
        "\n",
        "# Multi-query Attention\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, self.head_dim)\n",
        "        self.v_proj = nn.Linear(dim, self.head_dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, rotary_sin, rotary_cos):\n",
        "        B, T, C = x.shape\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).unsqueeze(1)  # shared keys\n",
        "        v = self.v_proj(x).unsqueeze(1)  # shared values\n",
        "\n",
        "        q = apply_rotary_pos_emb(q, rotary_sin, rotary_cos)\n",
        "        k = apply_rotary_pos_emb(k, rotary_sin, rotary_cos)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        out = (att @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiQueryAttention(dim, num_heads)\n",
        "        self.ln2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(dim * mlp_ratio), dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, rotary_sin, rotary_cos):\n",
        "        x = x + self.attn(self.ln1(x), rotary_sin, rotary_cos)\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Full model\n",
        "class ToyDeepSeekV3(nn.Module):\n",
        "    def __init__(self, vocab_size, dim, depth, num_heads, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(dim, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        x = self.token_emb(idx)\n",
        "        rotary_sin, rotary_cos = get_rotary_emb(self.max_seq_len, self.dim // self.blocks[0].attn.num_heads, x.device)\n",
        "        rotary_sin = rotary_sin[:T].unsqueeze(0).unsqueeze(0)\n",
        "        rotary_cos = rotary_cos[:T].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, rotary_sin, rotary_cos)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# Example use\n",
        "model = ToyDeepSeekV3(vocab_size=1000, dim=256, depth=4, num_heads=4, max_seq_len=128)\n",
        "dummy_input = torch.randint(0, 1000, (2, 128))\n",
        "out = model(dummy_input)  # (2, 128, 1000)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TriiMy6T-Fnm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}