{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVb/iabfzYRdmtvujxt8Xt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/CLIP_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xT-AtfbdPBsf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from pycocotools.coco import COCO\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "learning_rate = 0.0001\n",
        "feature_dim = 512  # Shared embedding space"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download MS COCO dataset (captions)\n",
        "# The file is too large, I cannot train in my limited google account\n",
        "coco_ann_file = './annotations/captions_train2017.json'\n",
        "coco_image_dir = './train2017/'\n",
        "\n",
        "# Initialize COCO API for caption annotations\n",
        "coco = COCO(coco_ann_file)\n",
        "\n",
        "# Load image ids and corresponding captions\n",
        "image_ids = coco.getImgIds()\n",
        "captions = coco.loadAnns(coco.getAnnIds(imgIds=image_ids))\n",
        "\n",
        "# Define a custom dataset for MS COCO\n",
        "class COCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, coco, image_dir, transform=None):\n",
        "        self.coco = coco\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = list(coco.imgs.keys())\n",
        "        self.annotations = coco.loadAnns(coco.getAnnIds(imgIds=self.image_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get the first caption for simplicity (MS COCO has 5 captions per image)\n",
        "        caption = self.annotations[idx]['caption']\n",
        "\n",
        "        return image, caption\n",
        "\n",
        "# Image transformations (ResNet requires 224x224 input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "coco_dataset = COCODataset(coco, coco_image_dir, transform=transform)\n",
        "data_loader = DataLoader(coco_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "jPRAfJQ6PKo4",
        "outputId": "55720ca9-2ba7-4b41-b8b4-2446243db5a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './annotations/captions_train2017.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d3c6497c8db3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize COCO API for caption annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco_ann_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load image ids and corresponding captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './annotations/captions_train2017.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Image Encoder (e.g., ResNet-18)\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.fc = nn.Identity()  # Remove final classification layer\n",
        "        self.projection = nn.Linear(512, feature_dim)  # Project features to shared space\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        return self.projection(features)\n",
        "\n",
        "# Text Encoder (e.g., BERT for captions)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.projection = nn.Linear(768, feature_dim)  # Project features to shared space\n",
        "\n",
        "    def forward(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=40)\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        outputs = self.bert(**inputs)\n",
        "        return self.projection(outputs.pooler_output)\n",
        "\n",
        "# CLIP Model (Image + Text Encoder)\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CLIPModel, self).__init__()\n",
        "        self.image_encoder = ImageEncoder().to(device)\n",
        "        self.text_encoder = TextEncoder().to(device)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        image_features = self.image_encoder(images)\n",
        "        text_features = self.text_encoder(captions)\n",
        "        return image_features, text_features\n",
        "\n",
        "# Loss Function: Contrastive Loss (Cross-entropy on cosine similarity)\n",
        "def contrastive_loss(image_features, text_features, temperature=0.07):\n",
        "    logits = torch.matmul(image_features, text_features.T) / temperature  # Compute cosine similarity\n",
        "    labels = torch.arange(len(logits)).to(device)\n",
        "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
        "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
        "    return (loss_i + loss_t) / 2\n"
      ],
      "metadata": {
        "id": "ysTrYn6pPN2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the model\n",
        "model = CLIPModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions in data_loader:\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        image_features, text_features = model(images, captions)\n",
        "\n",
        "        # Normalize features\n",
        "        image_features = nn.functional.normalize(image_features, dim=1)\n",
        "        text_features = nn.functional.normalize(text_features, dim=1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = contrastive_loss(image_features, text_features)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_loader):.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "73P5ypSuPRYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}