{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/paper_reading/blob/main/chapter_multilayer-perceptrons/mlp-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398126f3",
      "metadata": {
        "id": "398126f3"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc55a467",
      "metadata": {
        "id": "dc55a467"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l\n",
        "# !pip install d2l==1.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b31683",
      "metadata": {
        "origin_pos": 0,
        "id": "59b31683"
      },
      "source": [
        "# 多层感知机的从零开始实现\n",
        ":label:`sec_mlp_scratch`\n",
        "\n",
        "我们已经在 :numref:`sec_mlp`中描述了多层感知机（MLP），\n",
        "现在让我们尝试自己实现一个多层感知机。\n",
        "为了与之前softmax回归（ :numref:`sec_softmax_scratch` ）\n",
        "获得的结果进行比较，\n",
        "我们将继续使用Fashion-MNIST图像分类数据集\n",
        "（ :numref:`sec_fashion_mnist`）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbb0fc1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:21.394152Z",
          "iopub.status.busy": "2023-08-18T06:59:21.393407Z",
          "iopub.status.idle": "2023-08-18T06:59:24.364157Z",
          "shell.execute_reply": "2023-08-18T06:59:24.362977Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "ffbb0fc1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be61c4f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.369567Z",
          "iopub.status.busy": "2023-08-18T06:59:24.368990Z",
          "iopub.status.idle": "2023-08-18T06:59:24.501326Z",
          "shell.execute_reply": "2023-08-18T06:59:24.500151Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ],
        "id": "0be61c4f",
        "outputId": "d6b025ee-8aa7-4a36-b8c8-92a1c02335b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8236e2cd",
      "metadata": {
        "origin_pos": 6,
        "id": "8236e2cd"
      },
      "source": [
        "## 初始化模型参数\n",
        "\n",
        "回想一下，Fashion-MNIST中的每个图像由\n",
        "$28 \\times 28 = 784$个灰度像素值组成。\n",
        "所有图像共分为10个类别。\n",
        "忽略像素之间的空间结构，\n",
        "我们可以将每个图像视为具有784个输入特征\n",
        "和10个类的简单分类数据集。\n",
        "首先，我们将[**实现一个具有单隐藏层的多层感知机，\n",
        "它包含256个隐藏单元**]。\n",
        "注意，我们可以将这两个变量都视为超参数。\n",
        "通常，我们选择2的若干次幂作为层的宽度。\n",
        "因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。\n",
        "\n",
        "我们用几个张量来表示我们的参数。\n",
        "注意，对于每一层我们都要记录一个权重矩阵和一个偏置向量。\n",
        "跟以前一样，我们要为损失关于这些参数的梯度分配内存。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7730f280",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.508163Z",
          "iopub.status.busy": "2023-08-18T06:59:24.506257Z",
          "iopub.status.idle": "2023-08-18T06:59:24.520861Z",
          "shell.execute_reply": "2023-08-18T06:59:24.519861Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "7730f280"
      },
      "outputs": [],
      "source": [
        "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
        "\n",
        "W1 = nn.Parameter(torch.randn(\n",
        "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
        "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
        "W2 = nn.Parameter(torch.randn(\n",
        "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
        "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
        "\n",
        "params = [W1, b1, W2, b2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2700dfe8",
      "metadata": {
        "origin_pos": 11,
        "id": "2700dfe8"
      },
      "source": [
        "## 激活函数\n",
        "\n",
        "为了确保我们对模型的细节了如指掌，\n",
        "我们将[**实现ReLU激活函数**]，\n",
        "而不是直接调用内置的`relu`函数。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f46a813",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.528151Z",
          "iopub.status.busy": "2023-08-18T06:59:24.526356Z",
          "iopub.status.idle": "2023-08-18T06:59:24.533695Z",
          "shell.execute_reply": "2023-08-18T06:59:24.532654Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "5f46a813"
      },
      "outputs": [],
      "source": [
        "def relu(X):\n",
        "    a = torch.zeros_like(X)\n",
        "    return torch.max(X, a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "741dbe39",
      "metadata": {
        "origin_pos": 16,
        "id": "741dbe39"
      },
      "source": [
        "## 模型\n",
        "\n",
        "因为我们忽略了空间结构，\n",
        "所以我们使用`reshape`将每个二维图像转换为一个长度为`num_inputs`的向量。\n",
        "只需几行代码就可以(**实现我们的模型**)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d9923a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.541482Z",
          "iopub.status.busy": "2023-08-18T06:59:24.539621Z",
          "iopub.status.idle": "2023-08-18T06:59:24.547435Z",
          "shell.execute_reply": "2023-08-18T06:59:24.546468Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "b3d9923a"
      },
      "outputs": [],
      "source": [
        "def net(X):\n",
        "    X = X.reshape((-1, num_inputs))\n",
        "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
        "    return (H@W2 + b2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd600c14",
      "metadata": {
        "origin_pos": 21,
        "id": "bd600c14"
      },
      "source": [
        "## 损失函数\n",
        "\n",
        "由于我们已经从零实现过softmax函数（ :numref:`sec_softmax_scratch`），\n",
        "因此在这里我们直接使用高级API中的内置函数来计算softmax和交叉熵损失。\n",
        "回想一下我们之前在 :numref:`subsec_softmax-implementation-revisited`中\n",
        "对这些复杂问题的讨论。\n",
        "我们鼓励感兴趣的读者查看损失函数的源代码，以加深对实现细节的了解。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55fe0ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.554675Z",
          "iopub.status.busy": "2023-08-18T06:59:24.552824Z",
          "iopub.status.idle": "2023-08-18T06:59:24.560084Z",
          "shell.execute_reply": "2023-08-18T06:59:24.559049Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "f55fe0ea"
      },
      "outputs": [],
      "source": [
        "loss = nn.CrossEntropyLoss(reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a03c3a",
      "metadata": {
        "origin_pos": 25,
        "id": "b3a03c3a"
      },
      "source": [
        "## 训练\n",
        "\n",
        "幸运的是，[**多层感知机的训练过程与softmax回归的训练过程完全相同**]。\n",
        "可以直接调用`d2l`包的`train_ch3`函数（参见 :numref:`sec_softmax_scratch` ），\n",
        "将迭代周期数设置为10，并将学习率设置为0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py\n",
        "# d2l.train_ch6(net, train_iter, test_iter, loss, num_epochs, trainer)\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # No. of correct predictions, no. of predictions\n",
        "    metric = d2l.Accumulator(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                # Required for BERT Fine-tuning (to be covered later)\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer):\n",
        "  # lr device\n",
        "    \"\"\"Train a model with a GPU (defined in Chapter 6).\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    # print('training on', device)\n",
        "    # net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=trainer.param_groups[0]['lr'])\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            # X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (train_l, train_acc, None))\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "          f'test acc {test_acc:.3f}')\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "          f'on None')\n"
      ],
      "metadata": {
        "id": "ZBJ5DarjvI6I"
      },
      "id": "ZBJ5DarjvI6I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83cc0c7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T06:59:24.567796Z",
          "iopub.status.busy": "2023-08-18T06:59:24.566005Z",
          "iopub.status.idle": "2023-08-18T07:00:19.750339Z",
          "shell.execute_reply": "2023-08-18T07:00:19.748990Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "c83cc0c7",
        "outputId": "8996f221-84de-447f-fae8-fc3685ea1206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'parameters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-72cd21f06fdd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mupdater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_ch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdater\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-0814e7d043aa>\u001b[0m in \u001b[0;36mtrain_ch3\u001b[0;34m(net, train_iter, test_iter, loss, num_epochs, trainer)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# print('training on', device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# net.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
          ]
        }
      ],
      "source": [
        "num_epochs, lr = 10, 0.1\n",
        "updater = torch.optim.SGD(params, lr=lr)\n",
        "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
        "train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da98919",
      "metadata": {
        "origin_pos": 30,
        "id": "4da98919"
      },
      "source": [
        "为了对学习到的模型进行评估，我们将[**在一些测试数据上应用这个模型**]。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8230ba7c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:00:19.755336Z",
          "iopub.status.busy": "2023-08-18T07:00:19.754506Z",
          "iopub.status.idle": "2023-08-18T07:00:20.323813Z",
          "shell.execute_reply": "2023-08-18T07:00:20.322738Z"
        },
        "origin_pos": 31,
        "tab": [
          "pytorch"
        ],
        "id": "8230ba7c",
        "outputId": "f9d161cb-64ca-4d46-d050-e983152fd270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'd2l.torch' has no attribute 'predict_ch3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e2148c027a79>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_ch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'predict_ch3'"
          ]
        }
      ],
      "source": [
        "d2l.predict_ch3(net, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97420c6",
      "metadata": {
        "origin_pos": 32,
        "id": "c97420c6"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* 手动实现一个简单的多层感知机是很容易的。然而如果有大量的层，从零开始实现多层感知机会变得很麻烦（例如，要命名和记录模型的参数）。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 在所有其他参数保持不变的情况下，更改超参数`num_hiddens`的值，并查看此超参数的变化对结果有何影响。确定此超参数的最佳值。\n",
        "1. 尝试添加更多的隐藏层，并查看它对结果有何影响。\n",
        "1. 改变学习速率会如何影响结果？保持模型架构和其他超参数（包括轮数）不变，学习率设置为多少会带来最好的结果？\n",
        "1. 通过对所有超参数（学习率、轮数、隐藏层数、每层的隐藏单元数）进行联合优化，可以得到的最佳结果是什么？\n",
        "1. 描述为什么涉及多个超参数更具挑战性。\n",
        "1. 如果想要构建多个超参数的搜索方法，请想出一个聪明的策略。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0e00850",
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "id": "d0e00850"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1804)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}