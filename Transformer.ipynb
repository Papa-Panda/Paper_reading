{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+he0YetxBN0QXwI1dxe1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://chatgpt.com/c/67ecc4e3-2488-800e-af5d-1738e22c9257\n",
        "\n",
        "# https://gemini.google.com/app/d25e17f514b6a9d4\n",
        "# how is nn.MultiheadAttention implemented\n",
        "\n",
        "# my practice/experiments\n",
        "# https://colab.research.google.com/drive/1AmG5aDiyk0OrxnVcXey6Nd9ZxQzJuTBw#scrollTo=f_yJLgxxTXf5"
      ],
      "metadata": {
        "id": "eUWi1PkhlNl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMwxKLKFn-yD",
        "outputId": "147af104-f971-4a3e-f9be-45103b8b2bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 50257])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "\n",
        "class TransformerDecoderConfig(PretrainedConfig):\n",
        "    def __init__(self, vocab_size=50257, max_position_embeddings=1024,\n",
        "                 d_model=512, num_heads=8, num_layers=12, d_ff=2048,\n",
        "                 dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout = dropout\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # note layer norm is applied after self attention/FF\n",
        "        # word embedding is normalized already\n",
        "        # you can also set batch_first=True to avoid transpose\n",
        "        self.self_attn = nn.MultiheadAttention(config.d_model, config.num_heads, dropout=config.dropout)\n",
        "        self.norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(config.d_model, config.d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.d_ff, config.d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask):\n",
        "        attn_out, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoderModel(PreTrainedModel):\n",
        "    config_class = TransformerDecoderConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, config.max_position_embeddings, config.d_model))\n",
        "        self.layers = nn.ModuleList([TransformerDecoderBlock(config) for _ in range(config.num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_len = input_ids.size(1)\n",
        "        x = self.embedding(input_ids) + self.positional_encoding[:, :seq_len, :]\n",
        "        x = x.transpose(0, 1)  # (seq_len, batch, dim)\n",
        "\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))  # float mask\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.ln_f(x).transpose(0, 1)  # (batch, seq_len, dim)\n",
        "        return self.head(x)\n",
        "\n",
        "# Example usage:\n",
        "config = TransformerDecoderConfig()\n",
        "model = TransformerDecoderModel(config)\n",
        "input_ids = torch.randint(0, config.vocab_size, (1, 50))  # Single batch, 50 tokens\n",
        "output = model(input_ids)\n",
        "print(output.shape)  # Expected: (1, 50, vocab_size)"
      ]
    }
  ]
}