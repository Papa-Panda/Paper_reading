{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNfOoznAJXIf4yieWGO1cg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Parameter_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3hnzG5NLPlGn"
      },
      "outputs": [],
      "source": [
        "# parameter server\n",
        "# https://www.bilibili.com/read/cv16270512/?jump_opus=1\n",
        "# https://www.bilibili.com/video/BV1YA4y197G8/?spm_id_from=333.337.search-card.all.click\n",
        "# https://chatgpt.com/c/671cfd50-2910-800e-85e2-2502105d1391"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class ParameterServer:\n",
        "    def __init__(self, num_params):\n",
        "        # Initialize shared parameters with random values\n",
        "        self.parameters = np.random.rand(num_params)\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        # Return the current parameters (read-only)\n",
        "        with self.lock:\n",
        "            return self.parameters.copy()\n",
        "\n",
        "    def update_parameters(self, gradient):\n",
        "        # Update the parameters with the given gradient\n",
        "        with self.lock:\n",
        "            self.parameters -= 0.1 * gradient  # Simple gradient descent step\n",
        "            print(f\"Updated Parameters: {self.parameters}\")\n",
        "\n",
        "def worker(ps, worker_id):\n",
        "    \"\"\"Simulate a worker computing gradients and sending updates to the parameter server.\"\"\"\n",
        "    for epoch in range(3):\n",
        "        # Get the current parameters from the server\n",
        "        params = ps.get_parameters()\n",
        "        print(f\"Worker {worker_id} received parameters: {params}\")\n",
        "\n",
        "        # Simulate computing gradients (just using random values here)\n",
        "        gradient = np.random.rand(len(params))\n",
        "        print(f\"Worker {worker_id} computed gradient: {gradient}\")\n",
        "\n",
        "        # Send the gradient to the parameter server\n",
        "        ps.update_parameters(gradient)\n",
        "\n",
        "        # Simulate some computation delay\n",
        "        time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_workers = 3\n",
        "    num_params = 5  # Number of model parameters\n",
        "\n",
        "    # Create a Parameter Server instance\n",
        "    ps = ParameterServer(num_params)\n",
        "\n",
        "    # Launch multiple worker threads\n",
        "    threads = []\n",
        "    for i in range(num_workers):\n",
        "        t = threading.Thread(target=worker, args=(ps, i))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    print(\"Final Parameters:\", ps.get_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqQlyNXhPmcG",
        "outputId": "7aedf150-c868-4061-9a57-51251eae28e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 received parameters: [0.06988384 0.70016882 0.59833023 0.82530764 0.55903081]\n",
            "Worker 0 computed gradient: [0.48764011 0.93571694 0.90231022 0.42631718 0.20131776]\n",
            "Worker 1 received parameters: [0.06988384 0.70016882 0.59833023 0.82530764 0.55903081]\n",
            "Worker 1 computed gradient: [0.43749546 0.02488108 0.87034084 0.22645054 0.90254958]\n",
            "Updated Parameters: [0.02613429 0.69768071 0.51129614 0.80266258 0.46877585]\n",
            "Updated Parameters: [-0.02262972  0.60410902  0.42106512  0.76003086  0.44864407]\n",
            "Worker 2 received parameters: [-0.02262972  0.60410902  0.42106512  0.76003086  0.44864407]\n",
            "Worker 2 computed gradient: [0.11863408 0.36782503 0.76050471 0.12073516 0.31055374]\n",
            "Updated Parameters: [-0.03449313  0.56732652  0.34501465  0.74795735  0.4175887 ]\n",
            "Worker 0 received parameters: [-0.03449313  0.56732652  0.34501465  0.74795735  0.4175887 ]Worker 1 received parameters: [-0.03449313  0.56732652  0.34501465  0.74795735  0.4175887 ]\n",
            "\n",
            "Worker 0 computed gradient: [0.53112494 0.93208044 0.47224671 0.95155889 0.96222107]\n",
            "Updated Parameters: [-0.08760562  0.47411847  0.29778998  0.65280146  0.32136659]\n",
            "Worker 1 computed gradient: [0.19556667 0.7600585  0.87755851 0.06100916 0.07449791]\n",
            "Updated Parameters: [-0.10716229  0.39811262  0.21003413  0.64670054  0.3139168 ]\n",
            "Worker 2 received parameters: [-0.10716229  0.39811262  0.21003413  0.64670054  0.3139168 ]\n",
            "Worker 2 computed gradient: [0.38478215 0.82060879 0.53059689 0.66049369 0.5390201 ]\n",
            "Updated Parameters: [-0.14564051  0.31605174  0.15697444  0.58065117  0.26001479]\n",
            "Worker 0 received parameters: [-0.14564051  0.31605174  0.15697444  0.58065117  0.26001479]\n",
            "Worker 0 computed gradient: [0.90320399 0.66776504 0.32003602 0.94783539 0.66367619]\n",
            "Updated Parameters: [-0.2359609   0.24927524  0.12497084  0.48586764  0.19364717]\n",
            "Worker 1 received parameters: [-0.2359609   0.24927524  0.12497084  0.48586764  0.19364717]\n",
            "Worker 2 received parameters: [-0.2359609   0.24927524  0.12497084  0.48586764  0.19364717]\n",
            "Worker 2 computed gradient: [0.97373694 0.40012126 0.90982005 0.22699895 0.45048462]\n",
            "Updated Parameters: [-0.3333346   0.20926311  0.03398883  0.46316774  0.14859871]\n",
            "Worker 1 computed gradient: [0.63378251 0.33529063 0.61575376 0.08107604 0.90684752]\n",
            "Updated Parameters: [-0.39671285  0.17573405 -0.02758654  0.45506014  0.05791395]\n",
            "Final Parameters: [-0.39671285  0.17573405 -0.02758654  0.45506014  0.05791395]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import numpy as np\n",
        "from queue import Queue\n",
        "\n",
        "class ParameterServerNode:\n",
        "    \"\"\"A server node holding a partition of the parameters.\"\"\"\n",
        "    def __init__(self, param_slice):\n",
        "        self.parameters = param_slice\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Return a copy of the parameters for workers.\"\"\"\n",
        "        with self.lock:\n",
        "            return self.parameters.copy()\n",
        "\n",
        "    def update_parameters(self, gradient):\n",
        "        \"\"\"Update parameters with a gradient.\"\"\"\n",
        "        with self.lock:\n",
        "            self.parameters -= 0.1 * gradient  # Gradient descent step\n",
        "\n",
        "class ParameterServer:\n",
        "    \"\"\"Main parameter server that distributes parameter partitions.\"\"\"\n",
        "    def __init__(self, num_params, num_servers):\n",
        "        self.servers = []\n",
        "        # Partition parameters across multiple server nodes\n",
        "        partition_size = num_params // num_servers\n",
        "        for i in range(num_servers):\n",
        "            start = i * partition_size\n",
        "            end = (i + 1) * partition_size\n",
        "            param_slice = np.random.rand(partition_size)\n",
        "            self.servers.append(ParameterServerNode(param_slice))\n",
        "\n",
        "    def get_partition(self, server_id):\n",
        "        \"\"\"Get parameters from a specific server node.\"\"\"\n",
        "        return self.servers[server_id].get_parameters()\n",
        "\n",
        "    def update_partition(self, server_id, gradient):\n",
        "        \"\"\"Update parameters of a specific server node.\"\"\"\n",
        "        self.servers[server_id].update_parameters(gradient)\n",
        "\n",
        "class WorkerNode(threading.Thread):\n",
        "    \"\"\"Worker node that computes gradients and updates the server.\"\"\"\n",
        "    def __init__(self, worker_id, ps, num_epochs, server_id):\n",
        "        super().__init__()\n",
        "        self.worker_id = worker_id\n",
        "        self.ps = ps\n",
        "        self.num_epochs = num_epochs\n",
        "        self.server_id = server_id\n",
        "\n",
        "    def run(self):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Pull parameters from assigned server node\n",
        "            params = self.ps.get_partition(self.server_id)\n",
        "            print(f\"Worker {self.worker_id} pulled params: {params}\")\n",
        "\n",
        "            # Simulate gradient computation (random gradient)\n",
        "            gradient = np.random.rand(len(params))\n",
        "            print(f\"Worker {self.worker_id} computed gradient: {gradient}\")\n",
        "\n",
        "            # Push gradient update to server node\n",
        "            self.ps.update_partition(self.server_id, gradient)\n",
        "            print(f\"Worker {self.worker_id} pushed gradient.\")\n",
        "\n",
        "            # Simulate some delay to mimic real-world workload\n",
        "            time.sleep(1)\n",
        "\n",
        "def main():\n",
        "    num_params = 10  # Total number of parameters\n",
        "    num_servers = 2  # Number of server nodes (partitions)\n",
        "    num_workers = 3  # Number of worker nodes\n",
        "    num_epochs = 3   # Number of epochs per worker\n",
        "\n",
        "    # Initialize the Parameter Server\n",
        "    ps = ParameterServer(num_params, num_servers)\n",
        "\n",
        "    # Launch worker threads\n",
        "    workers = []\n",
        "    for i in range(num_workers):\n",
        "        # Assign each worker to a server partition (round-robin)\n",
        "        server_id = i % num_servers\n",
        "        worker = WorkerNode(i, ps, num_epochs, server_id)\n",
        "        workers.append(worker)\n",
        "        worker.start()\n",
        "\n",
        "    # Wait for all workers to finish\n",
        "    for worker in workers:\n",
        "        worker.join()\n",
        "\n",
        "    # Print final parameters from all server nodes\n",
        "    for i, server in enumerate(ps.servers):\n",
        "        print(f\"Final parameters on server {i}: {server.get_parameters()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_Xz9ygPpky",
        "outputId": "22ac58fe-39ba-450c-cede-13021de0e4c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 pulled params: [0.76674045 0.20679982 0.17874522 0.58535647 0.13439081]\n",
            "Worker 0 computed gradient: [0.50313609 0.84320696 0.24007032 0.93325103 0.37564311]\n",
            "Worker 0 pushed gradient.\n",
            "Worker 1 pulled params: [0.04819623 0.32114526 0.06134285 0.10630197 0.01038043]Worker 2 pulled params: [0.71642684 0.12247913 0.15473819 0.49203137 0.0968265 ]\n",
            "\n",
            "Worker 1 computed gradient: [0.09840055 0.52395605 0.05489859 0.99286323 0.69209853]\n",
            "Worker 1 pushed gradient.\n",
            "Worker 2 computed gradient: [0.09590854 0.57717189 0.66019709 0.43777671 0.76961479]\n",
            "Worker 2 pushed gradient.\n",
            "Worker 0 pulled params: [0.70683599 0.06476194 0.08871848 0.4482537  0.01986502]\n",
            "Worker 0 computed gradient: [0.89736703 0.60269473 0.41036854 0.66704673 0.5867233 ]\n",
            "Worker 0 pushed gradient.\n",
            "Worker 1 pulled params: [ 0.03835617  0.26874966  0.05585299  0.00701564 -0.05882943]\n",
            "Worker 1 computed gradient: [0.09379491 0.93000085 0.93766031 0.2367472  0.88286188]\n",
            "Worker 1 pushed gradient.\n",
            "Worker 2 pulled params: [ 0.61709928  0.00449246  0.04768163  0.38154903 -0.03880731]\n",
            "Worker 2 computed gradient: [0.22566792 0.85367298 0.79930851 0.86589007 0.17395153]\n",
            "Worker 2 pushed gradient.\n",
            "Worker 0 pulled params: [ 0.59453249 -0.08087483 -0.03224923  0.29496002 -0.05620246]\n",
            "Worker 0 computed gradient: [0.14846488 0.08990477 0.36349587 0.30112525 0.98448737]\n",
            "Worker 0 pushed gradient.\n",
            "Worker 1 pulled params: [ 0.02897668  0.17574957 -0.03791304 -0.01665908 -0.14711561]\n",
            "Worker 1 computed gradient: [0.32747142 0.23559684 0.9337963  0.84766685 0.70045195]\n",
            "Worker 1 pushed gradient.\n",
            "Worker 2 pulled params: [ 0.579686   -0.08986531 -0.06859881  0.26484749 -0.1546512 ]\n",
            "Worker 2 computed gradient: [0.89004548 0.41123772 0.65669926 0.18537562 0.11253975]\n",
            "Worker 2 pushed gradient.\n",
            "Final parameters on server 0: [ 0.49068145 -0.13098908 -0.13426874  0.24630993 -0.16590517]\n",
            "Final parameters on server 1: [-0.00377046  0.15218989 -0.13129267 -0.10142576 -0.21716081]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6fDhmCyQcb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}