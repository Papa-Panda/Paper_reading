{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2YmicHv4VMY76elMpQ+0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/DPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npqrtt1avZtN",
        "outputId": "26a8d682-6ec3-4220-ea47-89c69e436e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# 超参数\n",
        "LR_ACTOR = 0.001\n",
        "LR_CRITIC = 0.002\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005  # 软更新参数\n",
        "EPISODES = 200\n",
        "MAX_STEPS = 200\n",
        "\n",
        "# 创建环境\n",
        "env = gym.make(\"Pendulum-v1\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_bound = env.action_space.high[0]  # 作用于动作输出范围\n",
        "\n",
        "# 定义 Actor（确定性策略）\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_bound):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "        self.action_bound = action_bound  # 限制动作范围\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x)) * self.action_bound  # 输出范围 [-action_bound, action_bound]\n",
        "        return action\n",
        "\n",
        "# 定义 Critic（Q 值估计）\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)  # 状态和动作拼接\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_value = self.fc3(x)\n",
        "        return q_value\n",
        "\n",
        "# 初始化网络和优化器\n",
        "actor = Actor(state_dim, action_dim, action_bound)\n",
        "critic = Critic(state_dim, action_dim)\n",
        "target_actor = Actor(state_dim, action_dim, action_bound)\n",
        "target_critic = Critic(state_dim, action_dim)\n",
        "\n",
        "# 复制参数\n",
        "target_actor.load_state_dict(actor.state_dict())\n",
        "target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# 经验回放\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def push(self, transition):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "        return zip(*batch)\n",
        "\n",
        "buffer = ReplayBuffer()\n",
        "\n",
        "# 软更新函数\n",
        "def soft_update(target, source):\n",
        "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(TAU * source_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "# 训练 DPG\n",
        "batch_size = 64\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()[0]\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(MAX_STEPS):\n",
        "        with torch.no_grad():\n",
        "            action = actor(state).cpu().numpy()[0]\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        buffer.push((state, action, reward, next_state, done))\n",
        "\n",
        "        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if len(buffer.buffer) > batch_size:\n",
        "            # 采样数据\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "            states = torch.cat(states)\n",
        "            actions = torch.tensor(actions, dtype=torch.float32)\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "            # 计算目标 Q 值\n",
        "            with torch.no_grad():\n",
        "                next_actions = target_actor(next_states)  # 目标 Actor 计算下一步动作\n",
        "                target_q_values = target_critic(next_states, next_actions)  # 目标 Critic 计算 Q 值\n",
        "                target_q = rewards + GAMMA * target_q_values * (1 - dones)  # 计算目标 Q 值\n",
        "\n",
        "            # 更新 Critic 网络\n",
        "            critic_q_values = critic(states, actions)  # 当前 Critic 计算 Q 值\n",
        "            critic_loss = loss_fn(critic_q_values, target_q)  # 计算损失\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            # 更新 Actor 网络（策略梯度）\n",
        "            actor_loss = -critic(states, actor(states)).mean()  # 策略梯度\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            # 软更新目标网络\n",
        "            soft_update(target_actor, actor)\n",
        "            soft_update(target_critic, critic)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgRUe8prvaLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}