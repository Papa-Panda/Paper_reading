{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQBFRJbkq1UCsYFj+KKdbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/flow_NICE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look like: no scale"
      ],
      "metadata": {
        "id": "sXu10nwGpOjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original paper: https://arxiv.org/abs/1410.8516\n",
        "\n",
        "\n",
        "# discussion\n",
        "# 细水长flow之NICE：流模型的基本概念与实现\n",
        "# https://www.spaces.ac.cn/archives/5776\n",
        "\n",
        "\n",
        "# implementation\n",
        "# tried https://github.com/bojone/flow/blob/master/nice.py but not working\n",
        "# https://gemini.google.com/app/64f9247342835580"
      ],
      "metadata": {
        "id": "PRXujH5iSPBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # For checking if data directory exists"
      ],
      "metadata": {
        "id": "Pulj46Q2TVSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atBjOt1nTOWE",
        "outputId": "7c79ff8e-4a8f-40a8-be07-a53b4eafc1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Created data directory: ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 480kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.83MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.38MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/20, Loss: 544.4078\n",
            "Epoch 2/20, Loss: 179.5414\n",
            "Epoch 3/20, Loss: -170.1320\n",
            "Epoch 4/20, Loss: -490.5015\n",
            "Epoch 5/20, Loss: -757.3266\n",
            "Epoch 6/20, Loss: -961.7318\n",
            "Epoch 7/20, Loss: -1113.5070\n",
            "Epoch 8/20, Loss: -1224.7894\n",
            "Epoch 9/20, Loss: -1306.0871\n",
            "Epoch 10/20, Loss: -1364.8221\n",
            "Epoch 11/20, Loss: -1407.6398\n",
            "Epoch 12/20, Loss: -1445.0868\n",
            "Epoch 13/20, Loss: -1471.7054\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Data Loading and Preprocessing ---\n",
        "def load_mnist_data(batch_size=128):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the MNIST dataset.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): The batch size for the DataLoader.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (train_loader, test_loader).\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # Converts PIL Image to PyTorch Tensor and scales to [0, 1]\n",
        "        # Flatten the 28x28 image to a 784-dimensional vector\n",
        "        transforms.Lambda(lambda x: x.view(-1)),\n",
        "        # Add a small amount of uniform noise for dequantization.\n",
        "        # This is crucial for discrete data (like pixel values) when using\n",
        "        # continuous normalizing flows. It effectively spreads the discrete\n",
        "        # pixel values over a continuous interval [0, 1/256) for each pixel.\n",
        "        # transforms.Lambda(lambda x: x + torch.rand_like(x) / 256.0),\n",
        "        transforms.Lambda(lambda x: x + 0),\n",
        "        ])\n",
        "\n",
        "    # Ensure the data directory exists to store the downloaded MNIST dataset\n",
        "    data_dir = './data'\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "        print(f\"Created data directory: {data_dir}\")\n",
        "\n",
        "    # Load MNIST training and testing datasets\n",
        "    train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create DataLoaders for efficient batching and shuffling\n",
        "    # num_workers > 0 can speed up data loading, especially on GPU\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2 if torch.cuda.is_available() else 0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2 if torch.cuda.is_available() else 0)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# --- 2. Define the Neural Network for the 'm' function in Coupling Layer ---\n",
        "class CouplingNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple multi-layer perceptron (MLP) used within the Additive Coupling Layer.\n",
        "    This network learns the non-linear transformation 'm(x1)'.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initializes the CouplingNet.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimension of the input to this network (size of x1).\n",
        "            hidden_dim (int): Number of units in the hidden layers.\n",
        "            output_dim (int): Dimension of the output from this network (size of x2).\n",
        "        \"\"\"\n",
        "        super(CouplingNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(), # Activation function\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(), # Activation function\n",
        "            nn.Linear(hidden_dim, output_dim) # Output dimension matches the size of x2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the CouplingNet.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor (corresponding to x1 in the coupling layer).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor (corresponding to m(x1)).\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "# --- 3. Define the Additive Coupling Layer ---\n",
        "class AdditiveCouplingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    An additive coupling layer, a core component of the NICE model.\n",
        "    It splits the input into two parts, transforms one part based on the other,\n",
        "    and leaves the other part unchanged. This ensures invertibility and a\n",
        "    trivial Jacobian determinant.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, mask_config):\n",
        "        \"\"\"\n",
        "        Initializes an Additive Coupling Layer.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The total dimension of the input data.\n",
        "            hidden_dim (int): Hidden dimension for the internal CouplingNet.\n",
        "            mask_config (int): Determines how the input is split (0 for even indices as x1, 1 for odd).\n",
        "        \"\"\"\n",
        "        super(AdditiveCouplingLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.mask_config = mask_config\n",
        "\n",
        "        # Create a boolean mask to split the input for the current layer.\n",
        "        # If mask_config is 0, even-indexed dimensions are x1 (unmodified part),\n",
        "        # and odd-indexed dimensions are x2 (modified part).\n",
        "        # If mask_config is 1, odd-indexed dimensions are x1, and even-indexed are x2.\n",
        "        indices = torch.arange(input_dim)\n",
        "        if mask_config == 0:\n",
        "            mask = (indices % 2 == 0).float() # 1s for x1, 0s for x2\n",
        "        else:\n",
        "            mask = (indices % 2 != 0).float() # 1s for x1, 0s for x2\n",
        "\n",
        "        # Register the mask as a buffer so it's moved with the module to the correct device\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        # Determine the dimensions for x1 (input to CouplingNet) and x2 (output from CouplingNet).\n",
        "        # x1_dim is the number of elements that will be passed to the CouplingNet.\n",
        "        # x2_dim is the number of elements that will be modified by the CouplingNet's output.\n",
        "        x1_dim = int(self.mask.sum().item())\n",
        "        x2_dim = int((1 - self.mask).sum().item())\n",
        "\n",
        "        # Initialize the internal neural network (m function)\n",
        "        self.coupling_net = CouplingNet(x1_dim, hidden_dim, x2_dim)\n",
        "\n",
        "    def forward(self, x, log_det_jacobian):\n",
        "        \"\"\"\n",
        "        Forward pass of the additive coupling layer: y = f(x).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            log_det_jacobian (torch.Tensor): Accumulated log-determinant of the Jacobian from previous layers.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (output_tensor_y, updated_log_det_jacobian).\n",
        "        \"\"\"\n",
        "        # Ensure the mask is on the same device as the input tensor x\n",
        "        mask = self.mask.to(x.device)\n",
        "\n",
        "        # Split x into x1 (unmodified part) and x2 (part to be transformed)\n",
        "        x1_unmasked = x * mask # Contains x1 values and zeros for x2 positions\n",
        "        x2_unmasked = x * (1 - mask) # Contains x2 values and zeros for x1 positions\n",
        "\n",
        "        # Extract the active values for x1 to feed into the coupling network.\n",
        "        # `mask.bool()` creates a boolean mask for indexing.\n",
        "        x1_active = x1_unmasked[:, mask.bool()]\n",
        "\n",
        "        # Compute the output of the coupling network, m(x1)\n",
        "        coupling_output = self.coupling_net(x1_active)\n",
        "\n",
        "        # Apply the transformation: y2 = x2 + m(x1)\n",
        "        # We create a zero tensor with the same shape as x2_unmasked and fill\n",
        "        # the active (non-masked) positions with the transformed values.\n",
        "        y2_unmasked = torch.zeros_like(x2_unmasked)\n",
        "        y2_unmasked[:, (1 - mask).bool()] = x2_unmasked[:, (1 - mask).bool()] + coupling_output\n",
        "\n",
        "        # Combine y1 (which is just x1) and y2 to form the output y\n",
        "        y = x1_unmasked + y2_unmasked\n",
        "\n",
        "        # For additive coupling layers, the Jacobian determinant is 1, so log(det(J)) = 0.\n",
        "        # Thus, log_det_jacobian remains unchanged by this layer.\n",
        "        return y, log_det_jacobian\n",
        "\n",
        "    def inverse(self, y, log_det_jacobian):\n",
        "        \"\"\"\n",
        "        Inverse pass of the additive coupling layer: x = f_inverse(y).\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): Input tensor (output from forward pass).\n",
        "            log_det_jacobian (torch.Tensor): Accumulated log-determinant (not updated here).\n",
        "\n",
        "        Returns:\n",
        "            tuple: (output_tensor_x, unchanged_log_det_jacobian).\n",
        "        \"\"\"\n",
        "        # Ensure the mask is on the same device as the input tensor y\n",
        "        mask = self.mask.to(y.device)\n",
        "\n",
        "        # Split y into y1 (corresponding to x1) and y2 (corresponding to x2 + m(x1))\n",
        "        y1_unmasked = y * mask\n",
        "        y2_unmasked = y * (1 - mask)\n",
        "\n",
        "        # Extract active values for y1 to feed into the coupling network for inverse calculation\n",
        "        y1_active = y1_unmasked[:, mask.bool()]\n",
        "\n",
        "        # Compute m(y1)\n",
        "        coupling_output = self.coupling_net(y1_active)\n",
        "\n",
        "        # Apply the inverse transformation: x2 = y2 - m(y1)\n",
        "        x2_unmasked = torch.zeros_like(y2_unmasked)\n",
        "        x2_unmasked[:, (1 - mask).bool()] = y2_unmasked[:, (1 - mask).bool()] - coupling_output\n",
        "\n",
        "        # Combine y1 (which is x1) and x2 to form the original input x\n",
        "        x = y1_unmasked + x2_unmasked\n",
        "        return x, log_det_jacobian # log_det_jacobian is not updated during inverse for coupling layer\n",
        "\n",
        "# --- 4. Define the NICE Model (Flow) ---\n",
        "class NICE(nn.Module):\n",
        "    \"\"\"\n",
        "    The Non-linear Independent Components Estimation (NICE) model.\n",
        "    It comprises a stack of additive coupling layers followed by a diagonal scaling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_coupling_layers, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initializes the NICE model.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The dimension of the input data (e.g., 784 for MNIST).\n",
        "            num_coupling_layers (int): The number of additive coupling layers to stack.\n",
        "            hidden_dim (int): The hidden dimension for the CouplingNet within each layer.\n",
        "        \"\"\"\n",
        "        super(NICE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_coupling_layers = num_coupling_layers\n",
        "\n",
        "        # Create a list of additive coupling layers\n",
        "        self.coupling_layers = nn.ModuleList()\n",
        "        for i in range(num_coupling_layers):\n",
        "            # Alternate the mask configuration (0 or 1) for each coupling layer.\n",
        "            # This ensures that all dimensions of the input data are eventually\n",
        "            # transformed across the entire flow.\n",
        "            self.coupling_layers.append(AdditiveCouplingLayer(input_dim, hidden_dim, i % 2))\n",
        "\n",
        "        # Scaling layer: a learnable diagonal matrix applied at the end.\n",
        "        # `log_s` stores the log of the scaling factors.\n",
        "        # Initialized to zeros, meaning initial scaling factors are all 1 (exp(0)=1).\n",
        "        self.log_s = nn.Parameter(torch.zeros(input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the NICE model: transforms data x to latent variable z.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (latent_variable_z, total_log_det_jacobian).\n",
        "        \"\"\"\n",
        "        # Initialize log_det_jacobian for the batch.\n",
        "        # It accumulates the log-determinant of the Jacobian for the entire transformation.\n",
        "        log_det_jacobian = torch.zeros(x.size(0)).to(x.device)\n",
        "\n",
        "        # Pass through all additive coupling layers\n",
        "        for layer in self.coupling_layers:\n",
        "            x, log_det_jacobian = layer(x, log_det_jacobian)\n",
        "\n",
        "        # Apply the final diagonal scaling layer\n",
        "        # z = x * exp(log_s)\n",
        "        z = x * torch.exp(self.log_s)\n",
        "\n",
        "        # Add the log-determinant contribution from the scaling layer.\n",
        "        # For a diagonal matrix S with diagonal elements s_i, det(S) = product(s_i).\n",
        "        # So, log(det(S)) = sum(log(s_i)).\n",
        "        log_det_jacobian += self.log_s.sum()\n",
        "\n",
        "        return z, log_det_jacobian\n",
        "\n",
        "    def inverse(self, z):\n",
        "        \"\"\"\n",
        "        Inverse pass of the NICE model: transforms latent variable z back to data x.\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Latent variable tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Reconstructed data tensor x.\n",
        "        \"\"\"\n",
        "        # Inverse of the scaling layer: x_pre_scaling = z / exp(log_s)\n",
        "        x = z / torch.exp(self.log_s)\n",
        "\n",
        "        # Inverse through coupling layers in reverse order.\n",
        "        # For sampling, we typically don't need to compute the log-determinant\n",
        "        # during the inverse pass, so we pass `None`.\n",
        "        for layer in reversed(self.coupling_layers):\n",
        "            x, _ = layer.inverse(x, None) # log_det_jacobian is not needed for inverse here\n",
        "\n",
        "        return x\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"\n",
        "        Calculates the log-likelihood of the input data x under the NICE model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input data tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log-likelihood for each data point in the batch.\n",
        "        \"\"\"\n",
        "        # Transform data x to latent variable z and get the total log-determinant of Jacobian\n",
        "        z, log_det_jacobian = self.forward(x)\n",
        "\n",
        "        # Calculate log-probability of z under the prior distribution (standard Gaussian).\n",
        "        # The prior is assumed to be a standard multivariate Gaussian: N(0, I).\n",
        "        # log p(z) = -0.5 * torch.sum(z**2 + np.log(2 * np.pi), dim=1)\n",
        "        # Using torch.sum instead of np.log for consistency with torch tensors\n",
        "        log_prob_z = -0.5 * torch.sum(z**2 + torch.log(torch.tensor(2 * torch.pi)), dim=1)\n",
        "\n",
        "        # Apply the change of variables formula to get log p(x):\n",
        "        # log p(x) = log p(z) + log |det(df/dx)|\n",
        "        return log_prob_z + log_det_jacobian\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generates new data samples from the trained NICE model.\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): The number of samples to generate.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Generated data samples.\n",
        "        \"\"\"\n",
        "        # Sample from the prior (standard Gaussian) in the latent space\n",
        "        z = torch.randn(num_samples, self.input_dim).to(self.log_s.device)\n",
        "        # Transform these latent samples back to the data space using the inverse flow\n",
        "        x = self.inverse(z)\n",
        "        # Clamp values to [0, 1] as original MNIST pixels are in this range\n",
        "        # (after dequantization and normalization, values might slightly exceed 1)\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        return x\n",
        "\n",
        "# --- 5. Training Loop ---\n",
        "def train_nice_model(model, train_loader, optimizer, epochs, device):\n",
        "    \"\"\"\n",
        "    Trains the NICE model using the provided data loader and optimizer.\n",
        "\n",
        "    Args:\n",
        "        model (NICE): The NICE model instance.\n",
        "        train_loader (DataLoader): DataLoader for the training data.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
        "        epochs (int): Number of training epochs.\n",
        "        device (torch.device): Device to run the training on (CPU or CUDA).\n",
        "    \"\"\"\n",
        "    model.train() # Set the model to training mode\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            data = data.to(device) # Move data to the specified device\n",
        "\n",
        "            optimizer.zero_grad() # Clear gradients from previous step\n",
        "\n",
        "            # Calculate the log-likelihood of the data under the model\n",
        "            log_likelihood = model.log_prob(data)\n",
        "\n",
        "            # The loss is the negative mean log-likelihood (we want to maximize likelihood)\n",
        "            loss = -torch.mean(log_likelihood)\n",
        "\n",
        "            loss.backward() # Backpropagate to compute gradients\n",
        "            optimizer.step() # Update model parameters\n",
        "\n",
        "            total_loss += loss.item() # Accumulate loss for reporting\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "# --- 6. Generate Samples and Visualize ---\n",
        "def generate_samples(model, num_samples=64, img_size=(28, 28), show=True):\n",
        "    \"\"\"\n",
        "    Generates samples from the trained model and visualizes them.\n",
        "\n",
        "    Args:\n",
        "        model (NICE): The trained NICE model instance.\n",
        "        num_samples (int): Number of samples to generate.\n",
        "        img_size (tuple): Tuple (height, width) for reshaping samples back to image format.\n",
        "        show (bool): If True, displays the generated samples using matplotlib.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated samples.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode (disables dropout, batchnorm etc. if present)\n",
        "    print(f\"Generating {num_samples} samples...\")\n",
        "    with torch.no_grad(): # Disable gradient calculations during inference/sampling\n",
        "        samples = model.sample(num_samples).cpu() # Get samples and move them to CPU for plotting\n",
        "        # Reshape the flattened vectors back into image format (Batch, Channels, Height, Width)\n",
        "        samples = samples.view(-1, 1, img_size[0], img_size[1])\n",
        "\n",
        "        if show:\n",
        "            fig = plt.figure(figsize=(8, 8)) # Create a figure for plotting\n",
        "            # Determine grid size for subplots (e.g., 8x8 for 64 samples)\n",
        "            grid_size = int(np.sqrt(num_samples))\n",
        "            for i in range(num_samples):\n",
        "                ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "                # imshow expects (H, W) for grayscale, so squeeze the channel dimension\n",
        "                ax.imshow(samples[i].squeeze().numpy(), cmap='gray_r') # 'gray_r' for white background\n",
        "                ax.axis('off') # Turn off axes for cleaner image display\n",
        "            plt.suptitle('Generated MNIST Digits (NICE Model)') # Set overall title\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "            plt.show() # Display the plot\n",
        "        return samples\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Hyperparameters ---\n",
        "    INPUT_DIM = 28 * 28  # Dimension of flattened MNIST images (784 pixels)\n",
        "    HIDDEN_DIM = 1000   # Size of hidden layers within the CouplingNet (can be tuned)\n",
        "    NUM_COUPLING_LAYERS = 8 # Number of additive coupling layers in the flow (can be tuned)\n",
        "    BATCH_SIZE = 128    # Number of samples per training batch\n",
        "    EPOCHS = 20         # Number of training iterations over the entire dataset (can be increased)\n",
        "    LEARNING_RATE = 1e-3 # Learning rate for the Adam optimizer\n",
        "\n",
        "    # --- Device Configuration ---\n",
        "    # Check if CUDA (GPU) is available, otherwise use CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # --- Load Data ---\n",
        "    # Get training and testing data loaders\n",
        "    train_loader, _ = load_mnist_data(BATCH_SIZE)\n",
        "\n",
        "    # --- Initialize Model and Optimizer ---\n",
        "    # Create an instance of the NICE model and move it to the selected device\n",
        "    model = NICE(INPUT_DIM, NUM_COUPLING_LAYERS, HIDDEN_DIM).to(device)\n",
        "    # Use Adam optimizer for training the model parameters\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # --- Train the Model ---\n",
        "    train_nice_model(model, train_loader, optimizer, EPOCHS, device)\n",
        "\n",
        "    # --- Generate and Visualize Samples ---\n",
        "    # Generate 64 samples and display them\n",
        "    generate_samples(model, num_samples=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uc11T8HfTwgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}