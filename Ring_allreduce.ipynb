{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6viNEHl4kd70F3pHnDY0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/Ring_allreduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.bilibili.com/video/BV1mm42137X8/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=1fecee762931e992c96e5e166be13b76"
      ],
      "metadata": {
        "id": "p0dK8KL3QvpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gemini.google.com/app/bcd426a78e4d0c59"
      ],
      "metadata": {
        "id": "wgY48dqTRQy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "TphKo96DQrnT",
        "outputId": "73cdc4db-b5e2-420d-f282-a22adf56fdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spawning 4 processes for the conceptual ring allreduce example...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0604 04:46:49.151000 344 torch/multiprocessing/spawn.py:169] Terminating process 459 via signal SIGTERM\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ProcessExitedException",
          "evalue": "process 2 terminated with exit code 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be42a37873ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# and in environments like Colab.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-be42a37873ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Spawning {world_size} processes for the conceptual ring allreduce example...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# mp.spawn is used to launch multiple processes, each running 'run_process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     mp.spawn(run_process,\n\u001b[0m\u001b[1;32m    134\u001b[0m              \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m              \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    338\u001b[0m         )\n\u001b[1;32m    339\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout, grace_period)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 )\n\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    205\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProcessExitedException\u001b[0m: process 2 terminated with exit code 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import os\n",
        "import time\n",
        "\n",
        "def _do_ring_allreduce(rank, world_size, tensor):\n",
        "    \"\"\"\n",
        "    Performs a conceptual ring allreduce operation on a given tensor.\n",
        "    This function manually simulates the scatter-reduce and all-gather phases\n",
        "    using point-to-point communication.\n",
        "    \"\"\"\n",
        "    # Ensure the tensor is a float type for summation\n",
        "    tensor = tensor.float()\n",
        "\n",
        "    # Divide the tensor into chunks for the ring allreduce\n",
        "    # For simplicity, we assume the tensor size is divisible by world_size\n",
        "    chunk_size = tensor.numel() // world_size\n",
        "    # Reshape the tensor into chunks for easier indexing\n",
        "    chunks = [tensor.view(-1)[i * chunk_size:(i + 1) * chunk_size] for i in range(world_size)]\n",
        "\n",
        "    # Calculate left and right neighbors in the ring\n",
        "    left_neighbor = (rank - 1 + world_size) % world_size\n",
        "    right_neighbor = (rank + 1) % world_size\n",
        "\n",
        "    print(f\"Rank {rank}: Initial tensor: {tensor.tolist()}\")\n",
        "    print(f\"Rank {rank}: Initial chunks: {[c.tolist() for c in chunks]}\")\n",
        "    print(f\"Rank {rank}: Left neighbor: {left_neighbor}, Right neighbor: {right_neighbor}\")\n",
        "\n",
        "    # --- Phase 1: Scatter-Reduce ---\n",
        "    # Each process sends a chunk to its right neighbor and receives from its left.\n",
        "    # It accumulates the received chunk into its own corresponding chunk.\n",
        "    print(f\"\\nRank {rank}: Starting Scatter-Reduce Phase...\")\n",
        "    for i in range(world_size - 1):\n",
        "        # The chunk to send is (rank - i + world_size) % world_size\n",
        "        # The chunk to receive is (rank - i - 1 + world_size) % world_size\n",
        "        send_chunk_idx = (rank - i + world_size) % world_size\n",
        "        recv_chunk_idx = (rank - i - 1 + world_size) % world_size\n",
        "\n",
        "        # Create a buffer for receiving\n",
        "        recv_buffer = torch.empty_like(chunks[recv_chunk_idx])\n",
        "\n",
        "        # Send to right neighbor, receive from left neighbor\n",
        "        # Use non-blocking communication (isend/irecv) for better overlap\n",
        "        send_req = dist.isend(chunks[send_chunk_idx], dst=right_neighbor)\n",
        "        recv_req = dist.irecv(recv_buffer, src=left_neighbor)\n",
        "\n",
        "        # Wait for communication to complete\n",
        "        send_req.wait()\n",
        "        recv_req.wait()\n",
        "\n",
        "        # Accumulate the received data into the correct local chunk\n",
        "        chunks[recv_chunk_idx] += recv_buffer\n",
        "        print(f\"Rank {rank} (iter {i+1}): Received chunk {recv_chunk_idx} from {left_neighbor}, current chunks: {[c.tolist() for c in chunks]}\")\n",
        "        time.sleep(0.1) # Small delay for clearer output in Colab\n",
        "\n",
        "    print(f\"Rank {rank}: Scatter-Reduce Phase Complete. Chunks: {[c.tolist() for c in chunks]}\")\n",
        "\n",
        "    # --- Phase 2: All-Gather ---\n",
        "    # Each process sends its accumulated chunk to its right neighbor and receives\n",
        "    # a new chunk from its left neighbor, effectively gathering all the final\n",
        "    # reduced chunks.\n",
        "    print(f\"\\nRank {rank}: Starting All-Gather Phase...\")\n",
        "    for i in range(world_size - 1):\n",
        "        # The chunk to send is (rank - world_size + 1 + i + world_size) % world_size\n",
        "        # The chunk to receive is (rank - world_size + i + world_size) % world_size\n",
        "        send_chunk_idx = (rank - (world_size - 1) + i + world_size) % world_size\n",
        "        recv_chunk_idx = (rank - (world_size - 1) + i - 1 + world_size) % world_size\n",
        "\n",
        "        # Create a buffer for receiving\n",
        "        recv_buffer = torch.empty_like(chunks[recv_chunk_idx])\n",
        "\n",
        "        # Send to right neighbor, receive from left neighbor\n",
        "        send_req = dist.isend(chunks[send_chunk_idx], dst=right_neighbor)\n",
        "        recv_req = dist.irecv(recv_buffer, src=left_neighbor)\n",
        "\n",
        "        # Wait for communication to complete\n",
        "        send_req.wait()\n",
        "        recv_req.wait()\n",
        "\n",
        "        # Overwrite the received data into the correct local chunk (no accumulation)\n",
        "        chunks[recv_chunk_idx] = recv_buffer\n",
        "        print(f\"Rank {rank} (iter {i+1}): Received chunk {recv_chunk_idx} from {left_neighbor}, current chunks: {[c.tolist() for c in chunks]}\")\n",
        "        time.sleep(0.1) # Small delay for clearer output in Colab\n",
        "\n",
        "    print(f\"Rank {rank}: All-Gather Phase Complete. Chunks: {[c.tolist() for c in chunks]}\")\n",
        "\n",
        "    # Reconstruct the final tensor from the gathered chunks\n",
        "    final_tensor = torch.cat(chunks).view_as(tensor)\n",
        "    return final_tensor\n",
        "\n",
        "def run_process(rank, world_size):\n",
        "    \"\"\"\n",
        "    Function to be executed by each process.\n",
        "    Initializes the distributed environment and performs the ring allreduce.\n",
        "    \"\"\"\n",
        "    # Set environment variables for distributed communication\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '29500' # Use a consistent, available port\n",
        "\n",
        "    # Initialize the process group\n",
        "    # 'gloo' backend is suitable for CPU-based communication on a single machine.\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "    print(f\"Rank {rank} (out of {world_size}) process group initialized.\")\n",
        "\n",
        "    # Create an initial tensor for this rank\n",
        "    # Each element of the tensor is initialized with the rank ID\n",
        "    initial_data = torch.full((world_size * 2,), float(rank)) # Example: tensor of size 8 for world_size=4\n",
        "    print(f\"Rank {rank}: Initial data for allreduce: {initial_data.tolist()}\")\n",
        "\n",
        "    # Perform the custom ring allreduce\n",
        "    final_result = _do_ring_allreduce(rank, world_size, initial_data)\n",
        "\n",
        "    print(f\"\\nRank {rank}: Final tensor after custom ring allreduce: {final_result.tolist()}\")\n",
        "\n",
        "    # Verify the result against PyTorch's built-in all_reduce for comparison\n",
        "    # (This part is just for verification, not part of the custom ring allreduce itself)\n",
        "    torch_built_in_tensor = initial_data.clone()\n",
        "    dist.all_reduce(torch_built_in_tensor, op=dist.ReduceOp.SUM)\n",
        "    print(f\"Rank {rank}: Result from PyTorch's built-in all_reduce: {torch_built_in_tensor.tolist()}\")\n",
        "\n",
        "    # Clean up the process group\n",
        "    dist.destroy_process_group()\n",
        "    print(f\"Rank {rank}: Process group destroyed.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to spawn multiple processes for the distributed example.\n",
        "    \"\"\"\n",
        "    world_size = 4 # Number of simulated processes/nodes\n",
        "    print(f\"Spawning {world_size} processes for the conceptual ring allreduce example...\")\n",
        "    # mp.spawn is used to launch multiple processes, each running 'run_process'\n",
        "    mp.spawn(run_process,\n",
        "             args=(world_size,),\n",
        "             nprocs=world_size,\n",
        "             join=True) # 'join=True' makes the main process wait for all spawned processes to finish\n",
        "    print(\"\\nAll processes have completed the ring allreduce simulation.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This block ensures that the multiprocessing starts correctly on Windows\n",
        "    # and in environments like Colab.\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzD1gjkmQtIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}