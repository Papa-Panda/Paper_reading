{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnWE9zqTsK51+vGJE0+1Sq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papa-Panda/Paper_reading/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QGlCWc4ed3O8"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create some example documents\n",
        "documents = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Great Wall of China is over 13,000 miles long.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"The Northern and Southern Dynasties lasted from 420 to 589 AD.\",\n",
        "    \"Beijing is the capital of China.\"\n",
        "]\n",
        "\n",
        "# Step 2: Vectorize the documents using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "doc_vectors = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "# Step 3: Create a FAISS index for fast similarity search\n",
        "dimension = doc_vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_vectors)\n",
        "\n",
        "# Step 4: Define a function to retrieve the most relevant document\n",
        "def retrieve(query, k=1):\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "    return [documents[i] for i in indices[0]]\n",
        "\n",
        "# Step 5: Load a pre-trained text generation model (e.g., GPT-2)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Step 6: Define the RAG function\n",
        "def rag_generate(query):\n",
        "    retrieved_docs = retrieve(query, k=1)\n",
        "    context = \" \".join(retrieved_docs)  # Combine the retrieved document(s)\n",
        "\n",
        "    # Prepare the input for generation\n",
        "    input_text = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate the response\n",
        "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "EpOfEFMPd6ay",
        "outputId": "637ca530-65f6-4de2-f24c-3d21f289f227"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-25e018720033>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Test the RAG implementation\n",
        "# query = \"What is the capital of China?\"\n",
        "query = \"What is the capital of Spain?\"\n",
        "\n",
        "response = rag_generate(query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "rUT-H84Sd8_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WqfVDCBERfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a new embedding"
      ],
      "metadata": {
        "id": "gchZ6O54_qPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Load SBERT model for embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 2: Prepare example documents (knowledge base)\n",
        "documents = [\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The capital of Japan is Tokyo.\",\n",
        "    \"Python is a programming language loved by data scientists.\",\n",
        "    \"The Great Wall of China is a famous historical site.\",\n",
        "    \"Mount Everest is the tallest mountain in the world.\"\n",
        "]\n",
        "doc_embeddings = model.encode(documents, convert_to_tensor=True)  # Precompute embeddings\n",
        "\n",
        "# Step 3: RAG Function: Retrieve and Generate\n",
        "def rag(query, top_k=1):\n",
        "    # Encode query\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy().reshape(1, -1), doc_embeddings.cpu().numpy())\n",
        "\n",
        "    # Retrieve top-k relevant document(s)\n",
        "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "    retrieved_docs = [documents[idx] for idx in top_k_indices]\n",
        "\n",
        "    # Combine retrieved documents with query for generation\n",
        "    context = \" \".join(retrieved_docs)\n",
        "    return f\"Context: {context}\\nQuestion: {query}\\nAnswer: [Your generation logic here]\"\n",
        "\n",
        "# Step 4: Test the RAG implementation\n",
        "query = \"Where is the Eiffel Tower?\"\n",
        "response = rag(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "iygkC2fI_r84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vi7xnjD3_saw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}